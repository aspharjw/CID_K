{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io, os, sys, types\n",
    "from IPython import get_ipython\n",
    "from nbformat import read\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "def find_notebook(fullname, path=None):\n",
    "    \"\"\"find a notebook, given its fully qualified name and an optional path\n",
    "\n",
    "    This turns \"foo.bar\" into \"foo/bar.ipynb\"\n",
    "    and tries turning \"Foo_Bar\" into \"Foo Bar\" if Foo_Bar\n",
    "    does not exist.\n",
    "    \"\"\"\n",
    "    name = fullname.rsplit('.', 1)[-1]\n",
    "    if not path:\n",
    "        path = ['']\n",
    "    for d in path:\n",
    "        nb_path = os.path.join(d, name + \".ipynb\")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        # let import Notebook_Name find \"Notebook Name.ipynb\"\n",
    "        nb_path = nb_path.replace(\"_\", \" \")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        \n",
    "class NotebookLoader(object):\n",
    "    \"\"\"Module Loader for Jupyter Notebooks\"\"\"\n",
    "    def __init__(self, path=None):\n",
    "        self.shell = InteractiveShell.instance()\n",
    "        self.path = path\n",
    "\n",
    "    def load_module(self, fullname):\n",
    "        \"\"\"import a notebook as a module\"\"\"\n",
    "        path = find_notebook(fullname, self.path)\n",
    "\n",
    "        print (\"importing Jupyter notebook from %s\" % path)\n",
    "\n",
    "        # load the notebook object\n",
    "        with io.open(path, 'r', encoding='utf-8') as f:\n",
    "            nb = read(f, 4)\n",
    "\n",
    "\n",
    "        # create the module and add it to sys.modules\n",
    "        # if name in sys.modules:\n",
    "        #    return sys.modules[name]\n",
    "        mod = types.ModuleType(fullname)\n",
    "        mod.__file__ = path\n",
    "        mod.__loader__ = self\n",
    "        mod.__dict__['get_ipython'] = get_ipython\n",
    "        sys.modules[fullname] = mod\n",
    "\n",
    "        # extra work to ensure that magics that would affect the user_ns\n",
    "        # actually affect the notebook module's ns\n",
    "        save_user_ns = self.shell.user_ns\n",
    "        self.shell.user_ns = mod.__dict__\n",
    "\n",
    "        try:\n",
    "          for cell in nb.cells:\n",
    "            if cell.cell_type == 'code':\n",
    "                # transform the input to executable Python\n",
    "                code = self.shell.input_transformer_manager.transform_cell(cell.source)\n",
    "                # run the code in themodule\n",
    "                exec(code, mod.__dict__)\n",
    "        finally:\n",
    "            self.shell.user_ns = save_user_ns\n",
    "        return mod\n",
    "    \n",
    "class NotebookFinder(object):\n",
    "    \"\"\"Module finder that locates Jupyter Notebooks\"\"\"\n",
    "    def __init__(self):\n",
    "        self.loaders = {}\n",
    "\n",
    "    def find_module(self, fullname, path=None):\n",
    "        nb_path = find_notebook(fullname, path)\n",
    "        if not nb_path:\n",
    "            return\n",
    "\n",
    "        key = path\n",
    "        if path:\n",
    "            # lists aren't hashable\n",
    "            key = os.path.sep.join(path)\n",
    "\n",
    "        if key not in self.loaders:\n",
    "            self.loaders[key] = NotebookLoader(path)\n",
    "        return self.loaders[key]\n",
    "    \n",
    "sys.meta_path.append(NotebookFinder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from rnn.ipynb\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable   \n",
    "from torch import optim\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../Preprocessor\")\n",
    "import format_module\n",
    "reviewDB = format_module.FormattedReview.reviewDB\n",
    "\n",
    "import rnn\n",
    "import naivebayesian\n",
    "import cnn\n",
    "import conclude\n",
    "import mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_object(filename):\n",
    "    with open(filename, 'rb') as input:\n",
    "        return pickle.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class classifierModule(nn.Module):\n",
    "    def __init__(self, input_size, batch_size):\n",
    "        super(classifierModule, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.rnn_model = rnn.RNN_model(input_size)\n",
    "        self.rnn_out_size = rnn.RNN_model.hidden_size\n",
    "        self.rnn_mlp = mlp.mlp(format_module.FormattedReview.attribute_num, self.rnn_out_size)\n",
    "        \n",
    "        self.nb_model = naivebayesian.NaiveBayesianDB()\n",
    "        self.nb_out_size = 1\n",
    "        self.nb_mlp = mlp.mlp(format_module.FormattedReview.attribute_num, self.nb_out_size)\n",
    "        \n",
    "        self.cnn_model = cnn.ConvNet(input_size)\n",
    "        self.cnn_out_size = cnn.ConvNet.output_vector_size\n",
    "        self.cnn_mlp = mlp.mlp(format_module.FormattedReview.attribute_num, self.cnn_out_size)\n",
    "        \n",
    "        self.conclude = conclude.conclude()\n",
    "        \n",
    "    def encoder(self, formattedList):\n",
    "        length = len(formattedList)\n",
    "        contextList = [formattedList[i].context for i in range(length)]\n",
    "\n",
    "        lengths = torch.LongTensor([len(contextList[i]) for i in range(length)])\n",
    "        max_len = torch.max(lengths)\n",
    "        \n",
    "        data = np.zeros((length, max_len, self.input_size))\n",
    "\n",
    "        for i in range(length):\n",
    "            context = contextList[i]\n",
    "            if not (context.size == 0):\n",
    "                data[i, :context.shape[0],:] = context\n",
    "            else:\n",
    "                lengths[i] = 1\n",
    "            \n",
    "        return self.sort_batch(torch.FloatTensor(data), formattedList, lengths)\n",
    "        \n",
    "    def sort_batch(self, context, formatted, seq_len):\n",
    "        batch_size = context.size(0)\n",
    "        sorted_seq_len, sorted_idx = seq_len.sort(0, descending = True)\n",
    "        \n",
    "        sorted_context = context[sorted_idx]\n",
    "        sorted_formatted = [formatted[i] for i in sorted_idx]\n",
    "\n",
    "        for f in sorted_formatted:\n",
    "            print(len(f.context))\n",
    "        \n",
    "        return Variable(sorted_context), sorted_formatted, sorted_seq_len\n",
    "    \n",
    "    def resize_input(self, input):\n",
    "        list_ = list()\n",
    "        for i in range(0, len(input), self.batch_size):\n",
    "            list_.append(input[i:i+self.batch_size])\n",
    "        return list_\n",
    "        \n",
    "    def forward(self, formatted_list, hidden=None):\n",
    "        context, formatted, lengths = self.encoder(bl)\n",
    "        \n",
    "        rnn_out = self.rnn_model(context, lengths)\n",
    "        cnn_out = self.cnn_model(context)\n",
    "        nb_out = self.nb_model.naive_bayes_FRlist(formatted)\n",
    "        \n",
    "        '''\n",
    "        print(\"rnn_out : \", rnn_out, \"\\n\\n\\n\")\n",
    "        print(\"cnn_out : \", cnn_out, \"\\n\\n\\n\")\n",
    "        print(\"nb_out : \", nb_out, \"\\n\\n\\n\")\n",
    "        '''\n",
    "        \n",
    "        rnn_mlp_out = self.rnn_mlp(self.rnn_mlp.getdata(formatted, rnn_out))\n",
    "        cnn_mlp_out = self.cnn_mlp(self.cnn_mlp.getdata(formatted, cnn_out))\n",
    "        nb_mlp_out = self.nb_mlp(self.nb_mlp.getdata(formatted, nb_out))\n",
    "        \n",
    "        return self.conclude(self.conclude.bind(rnn_mlp_out, cnn_mlp_out, nb_mlp_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137\n",
      "120\n",
      "52\n",
      "42\n",
      "34\n",
      "32\n",
      "27\n",
      "25\n",
      "20\n",
      "9\n",
      "Variable containing:\n",
      " 0.3501  0.6499\n",
      " 0.3485  0.6515\n",
      " 0.3545  0.6455\n",
      " 0.3531  0.6469\n",
      " 0.3476  0.6524\n",
      " 0.3488  0.6512\n",
      " 0.3604  0.6396\n",
      " 0.3592  0.6408\n",
      " 0.3509  0.6491\n",
      " 0.3574  0.6426\n",
      "[torch.FloatTensor of size 10x2]\n",
      "\n",
      "141\n",
      "124\n",
      "104\n",
      "71\n",
      "63\n",
      "33\n",
      "27\n",
      "25\n",
      "19\n",
      "7\n",
      "Variable containing:\n",
      " 0.3505  0.6495\n",
      " 0.3470  0.6530\n",
      " 0.3491  0.6509\n",
      " 0.3518  0.6482\n",
      " 0.3472  0.6528\n",
      " 0.3522  0.6478\n",
      " 0.3534  0.6466\n",
      " 0.3438  0.6562\n",
      " 0.3597  0.6403\n",
      " 0.3592  0.6408\n",
      "[torch.FloatTensor of size 10x2]\n",
      "\n",
      "112\n",
      "85\n",
      "83\n",
      "54\n",
      "45\n",
      "28\n",
      "22\n",
      "20\n",
      "13\n",
      "10\n",
      "Variable containing:\n",
      " 0.3527  0.6473\n",
      " 0.3433  0.6567\n",
      " 0.3509  0.6491\n",
      " 0.3504  0.6496\n",
      " 0.3452  0.6548\n",
      " 0.3567  0.6433\n",
      " 0.3498  0.6502\n",
      " 0.3603  0.6397\n",
      " 0.3491  0.6509\n",
      " 0.3545  0.6455\n",
      "[torch.FloatTensor of size 10x2]\n",
      "\n",
      "86\n",
      "47\n",
      "46\n",
      "33\n",
      "27\n",
      "25\n",
      "15\n",
      "13\n",
      "8\n",
      "6\n",
      "Variable containing:\n",
      " 0.3558  0.6442\n",
      " 0.3540  0.6460\n",
      " 0.3814  0.6186\n",
      " 0.3469  0.6531\n",
      " 0.3482  0.6518\n",
      " 0.3599  0.6401\n",
      " 0.3606  0.6394\n",
      " 0.3634  0.6366\n",
      " 0.3561  0.6439\n",
      " 0.3570  0.6430\n",
      "[torch.FloatTensor of size 10x2]\n",
      "\n",
      "96\n",
      "73\n",
      "68\n",
      "49\n",
      "41\n",
      "28\n",
      "16\n",
      "15\n",
      "15\n",
      "7\n",
      "Variable containing:\n",
      " 0.3535  0.6465\n",
      " 0.3558  0.6442\n",
      " 0.3596  0.6404\n",
      " 0.3627  0.6373\n",
      " 0.3557  0.6443\n",
      " 0.3522  0.6478\n",
      " 0.3598  0.6402\n",
      " 0.3491  0.6509\n",
      " 0.3599  0.6401\n",
      " 0.3539  0.6461\n",
      "[torch.FloatTensor of size 10x2]\n",
      "\n",
      "88\n",
      "83\n",
      "70\n",
      "55\n",
      "48\n",
      "43\n",
      "43\n",
      "27\n",
      "20\n",
      "12\n",
      "Variable containing:\n",
      " 0.3567  0.6433\n",
      " 0.3593  0.6407\n",
      " 0.3498  0.6502\n",
      " 0.3479  0.6521\n",
      " 0.3505  0.6495\n",
      " 0.3508  0.6492\n",
      " 0.3479  0.6521\n",
      " 0.3540  0.6460\n",
      " 0.3556  0.6444\n",
      " 0.3633  0.6367\n",
      "[torch.FloatTensor of size 10x2]\n",
      "\n",
      "162\n",
      "94\n",
      "94\n",
      "44\n",
      "31\n",
      "26\n",
      "25\n",
      "22\n",
      "17\n",
      "0\n",
      "Variable containing:\n",
      " 0.3470  0.6530\n",
      " 0.3527  0.6473\n",
      " 0.3575  0.6425\n",
      " 0.3545  0.6455\n",
      " 0.3572  0.6428\n",
      " 0.3550  0.6450\n",
      " 0.3445  0.6555\n",
      " 0.3571  0.6429\n",
      " 0.3482  0.6518\n",
      " 0.3837  0.6163\n",
      "[torch.FloatTensor of size 10x2]\n",
      "\n",
      "71\n",
      "64\n",
      "46\n",
      "44\n",
      "44\n",
      "41\n",
      "31\n",
      "28\n",
      "22\n",
      "8\n",
      "Variable containing:\n",
      " 0.3569  0.6431\n",
      " 0.3561  0.6439\n",
      " 0.3491  0.6509\n",
      " 0.3516  0.6484\n",
      " 0.3485  0.6515\n",
      " 0.3506  0.6494\n",
      " 0.3561  0.6439\n",
      " 0.3609  0.6391\n",
      " 0.3497  0.6503\n",
      " 0.3621  0.6379\n",
      "[torch.FloatTensor of size 10x2]\n",
      "\n",
      "120\n",
      "89\n",
      "79\n",
      "77\n",
      "57\n",
      "51\n",
      "27\n",
      "23\n",
      "14\n",
      "0\n",
      "Variable containing:\n",
      " 0.3533  0.6467\n",
      " 0.3552  0.6448\n",
      " 0.3538  0.6462\n",
      " 0.3528  0.6472\n",
      " 0.3582  0.6418\n",
      " 0.3527  0.6473\n",
      " 0.3549  0.6451\n",
      " 0.3631  0.6369\n",
      " 0.3485  0.6515\n",
      " 0.3797  0.6203\n",
      "[torch.FloatTensor of size 10x2]\n",
      "\n",
      "112\n",
      "86\n",
      "85\n",
      "53\n",
      "42\n",
      "35\n",
      "24\n",
      "22\n",
      "20\n",
      "14\n",
      "Variable containing:\n",
      " 0.3516  0.6484\n",
      " 0.3476  0.6524\n",
      " 0.3549  0.6451\n",
      " 0.3549  0.6451\n",
      " 0.3500  0.6500\n",
      " 0.3507  0.6493\n",
      " 0.3475  0.6525\n",
      " 0.3519  0.6481\n",
      " 0.3546  0.6454\n",
      " 0.3568  0.6432\n",
      "[torch.FloatTensor of size 10x2]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor param in test_classifier.parameters():\\n     print(type(param.data), param.size())\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_list = load_object(\"../Preprocessor/save_formatted_review.pkl\")\n",
    "test_classifier = classifierModule(100, 10)\n",
    "\n",
    "test_classifier.nb_model.add_FRlist(formatted_list)\n",
    "\n",
    "batch_list = test_classifier.resize_input(formatted_list)\n",
    "for bl in batch_list:\n",
    "    print(test_classifier(bl))\n",
    "\n",
    "'''\n",
    "for param in test_classifier.parameters():\n",
    "     print(type(param.data), param.size())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.6659031   1.          0.          0.          0.68713822  0.85714286\n",
      "  0.        ]\n",
      "[ 0.54028185  1.          0.          0.          0.54963984  0.85714286\n",
      "  0.        ]\n",
      "[ 0.57978587  1.          0.          0.          0.80022876  0.71428571\n",
      "  0.        ]\n",
      "[ 0.63541954  0.2         0.          0.          0.00063837  0.57142857\n",
      "  0.        ]\n",
      "[ 0.26519823  1.          0.          0.          0.83260339  0.14285714\n",
      "  0.        ]\n",
      "[ 1.00000003  1.          0.          0.          0.80869791  0.14285714\n",
      "  0.        ]\n",
      "[ 0.3131324   0.1         0.          0.          0.69642361  0.28571429\n",
      "  0.        ]\n",
      "[ 0.99999996  0.2         0.          0.          0.0079967   0.71428571\n",
      "  0.        ]\n",
      "[ 0.53118923  0.3         0.          0.          0.93098618  0.          0.        ]\n",
      "[ 0.70273454  0.2         0.          0.          0.54457049  0.85714286\n",
      "  0.        ]\n",
      "[ 0.9999999   0.2         0.          0.          0.97971975  0.85714286\n",
      "  0.        ]\n",
      "[ 0.2280304   0.8         0.          0.          0.02989583  0.14285714\n",
      "  0.        ]\n",
      "[ 1.00000016  1.          0.          0.          0.6441539   0.85714286\n",
      "  0.        ]\n",
      "[ 1.00000003  1.          0.          0.          0.51730573  0.          0.        ]\n",
      "[ 0.74059727  0.1         0.          0.          0.09203576  0.57142857\n",
      "  0.        ]\n",
      "[ 0.99999999  0.2         0.          0.          0.75136488  0.28571429\n",
      "  0.        ]\n",
      "[ 0.28811371  1.          0.          0.          0.50621363  0.14285714\n",
      "  0.        ]\n",
      "[ 0.43285143  1.          0.          0.          0.39938569  0.57142857\n",
      "  0.        ]\n",
      "[ 0.99999995  1.          0.          0.          0.71512828  0.28571429\n",
      "  0.        ]\n",
      "[ 0.99999997  0.1         0.          0.          0.85313058  0.42857143\n",
      "  0.        ]\n",
      "[ 0.44409099  0.1         0.          0.          0.69113769  0.          0.        ]\n",
      "[ 0.99999998  0.8         0.          0.          0.85523505  0.          0.        ]\n",
      "[ 1.00000012  1.          0.          0.          0.86551969  0.14285714\n",
      "  0.        ]\n",
      "[ 0.99999994  0.2         0.          0.          0.77045137  0.85714286\n",
      "  0.        ]\n",
      "[ 0.42189092  1.          0.          0.          0.68236965  0.71428571\n",
      "  0.        ]\n",
      "[ 1.00000004  0.9         0.          0.          0.48205953  0.85714286\n",
      "  0.        ]\n",
      "[ 0.99999999  1.          0.          0.          0.50266204  0.71428571\n",
      "  0.        ]\n",
      "[ 1.00000006  1.          0.          0.          0.92647635  0.          0.        ]\n",
      "[ 0.28925741  1.          0.          0.          0.01758123  0.57142857\n",
      "  0.        ]\n",
      "[ 0.24333654  0.9         0.          0.          0.74452799  0.28571429\n",
      "  0.        ]\n",
      "[ 0.33674538  1.          0.          0.          0.67547983  0.          0.        ]\n",
      "[ 0.26620488  0.1         0.          0.          0.38570705  0.71428571\n",
      "  0.        ]\n",
      "[ 0.42121813  0.8         0.          0.          0.57342664  0.71428571\n",
      "  0.        ]\n",
      "[ 0.33922169  1.          0.          0.          0.79945216  0.71428571\n",
      "  0.        ]\n",
      "[ 0.35827205  1.          0.          0.          0.70528078  0.14285714\n",
      "  0.        ]\n",
      "[ 1.00000009  0.8         0.          0.          0.68155041  0.14285714\n",
      "  0.        ]\n",
      "[ 0.42900783  1.          0.          0.          0.97384259  0.85714286\n",
      "  0.        ]\n",
      "[ 0.99999997  1.          0.          0.          0.81670237  0.85714286\n",
      "  0.        ]\n",
      "[ 0.63768753  0.4         0.          0.          0.60734634  0.42857143\n",
      "  0.        ]\n",
      "[-1.          1.          0.          0.          0.87733304  0.14285714\n",
      "  0.        ]\n",
      "[ 0.41875847  1.          0.          0.          0.72430392  0.71428571\n",
      "  0.        ]\n",
      "[ 0.49503472  1.          0.          0.          0.54172403  0.57142857\n",
      "  0.        ]\n",
      "[ 0.46868977  0.          0.          0.          0.94525463  0.71428571\n",
      "  0.        ]\n",
      "[ 1.00000011  0.9         0.          0.          0.60062584  0.42857143\n",
      "  0.        ]\n",
      "[ 0.41442486  0.2         0.          0.          0.91525374  0.85714286\n",
      "  0.        ]\n",
      "[ 0.59811889  0.2         0.          0.          0.76244905  0.          0.        ]\n",
      "[ 1.00000004  1.          0.          0.          0.06967777  0.          0.        ]\n",
      "[ 0.4048324   0.2         0.          0.          0.85153715  0.28571429\n",
      "  0.        ]\n",
      "[ 0.70790733  1.          0.          0.          0.9492512   0.71428571\n",
      "  0.        ]\n",
      "[ 0.13188867  1.          0.          0.          0.84558456  0.14285714\n",
      "  0.        ]\n",
      "[ 1.00000009  1.          0.          0.          0.844212    0.28571429\n",
      "  0.        ]\n",
      "[ 0.99999999  1.          0.          0.          0.74425926  0.42857143\n",
      "  0.        ]\n",
      "[ 0.25250619  0.2         0.          0.          0.93054758  0.28571429\n",
      "  0.        ]\n",
      "[ 0.58868099  1.          0.          0.          0.70346851  0.71428571\n",
      "  0.        ]\n",
      "[ 0.70280687  0.2         0.          0.          0.86884255  0.42857143\n",
      "  0.        ]\n",
      "[ 0.55040368  1.          0.          0.          0.93995233  0.14285714\n",
      "  0.        ]\n",
      "[ 1.00000005  1.          0.          0.          0.32595353  0.28571429\n",
      "  0.        ]\n",
      "[ 1.          1.          0.          0.          0.10269676  0.57142857\n",
      "  0.        ]\n",
      "[ 1.00000011  1.          0.          0.          0.6336933   0.85714286\n",
      "  0.        ]\n",
      "[ 0.55144307  0.2         0.          0.          0.6568633   0.57142857\n",
      "  0.        ]\n",
      "[ 0.47594183  1.          0.          0.          0.8140649   0.71428571\n",
      "  0.        ]\n",
      "[ 0.22999832  1.          0.          0.          0.95776627  0.71428571\n",
      "  0.        ]\n",
      "[ 0.63638961  0.2         0.          0.          0.62522505  0.28571429\n",
      "  0.        ]\n",
      "[ 1.00000003  1.          0.          0.          0.65841034  0.71428571\n",
      "  0.        ]\n",
      "[ 0.34999817  0.2         0.          0.          0.79539931  0.          0.        ]\n",
      "[ 0.32889923  1.          0.          0.          0.71092715  0.42857143\n",
      "  0.        ]\n",
      "[ 1.00000005  1.          0.          0.          0.88883847  0.14285714\n",
      "  0.        ]\n",
      "[ 1.00000009  1.          0.          0.          0.81213962  0.28571429\n",
      "  0.        ]\n",
      "[-1.          0.6         0.          0.          0.50490296  0.14285714\n",
      "  0.        ]\n",
      "[ 0.55367979  0.2         0.          0.          0.86130935  0.71428571\n",
      "  0.        ]\n",
      "[ 0.47634014  0.8         0.          0.          0.5528794   0.71428571\n",
      "  0.        ]\n",
      "[ 0.61175146  0.2         0.          0.          0.63414863  0.28571429\n",
      "  0.        ]\n",
      "[ 1.00000016  1.          0.          0.01593612  0.8295994   0.28571429\n",
      "  0.        ]\n",
      "[ 1.00000016  0.8         0.          0.          0.66275207  0.          0.        ]\n",
      "[ 0.60343077  0.2         0.          0.          0.92954273  0.          0.        ]\n",
      "[ 0.17585899  1.          0.          0.          0.38391074  0.          0.        ]\n",
      "[ 0.99999997  0.4         0.          0.          0.89109267  0.71428571\n",
      "  0.        ]\n",
      "[ 0.19940347  0.6         0.          0.          0.50416667  0.42857143\n",
      "  0.        ]\n",
      "[ 0.9999999   1.          0.          0.          0.70218039  0.42857143\n",
      "  0.        ]\n",
      "[ 1.00000001  0.9         0.          0.          0.43330419  0.57142857\n",
      "  0.        ]\n",
      "[ 0.23221985  0.2         0.          0.          0.5514186   0.          0.        ]\n",
      "[ 0.51343538  1.          0.          0.          0.62041707  0.57142857\n",
      "  0.        ]\n",
      "[ 0.40007053  1.          0.          0.          0.04031139  0.57142857\n",
      "  0.        ]\n",
      "[ 0.68314922  0.2         0.          0.          0.57266278  0.28571429\n",
      "  0.        ]\n",
      "[ 0.6628223   1.          0.          0.          0.85604848  0.28571429\n",
      "  0.        ]\n",
      "[ 0.66894133  0.1         0.          0.          0.49723267  0.42857143\n",
      "  0.        ]\n",
      "[ 0.52031597  1.          0.          0.          0.38945602  0.28571429\n",
      "  0.        ]\n",
      "[-1.          1.          0.          0.          0.66793057  0.71428571\n",
      "  0.        ]\n",
      "[ 0.38349678  1.          0.          0.          0.72505728  0.71428571\n",
      "  0.        ]\n",
      "[ 0.78959527  1.          0.          0.          0.43668098  0.71428571\n",
      "  0.        ]\n",
      "[ 0.9999999   1.          0.          0.          0.40536881  0.42857143\n",
      "  0.        ]\n",
      "[ 0.3850252   0.9         0.          0.          0.63279938  0.85714286\n",
      "  0.        ]\n",
      "[ 0.62636932  0.2         0.          0.          0.50498253  0.14285714\n",
      "  0.        ]\n",
      "[ 0.66785038  1.          0.          0.          0.67020639  0.57142857\n",
      "  0.        ]\n",
      "[ 0.52443614  1.          0.          0.          0.84855152  0.57142857\n",
      "  0.        ]\n",
      "[ 1.00000007  1.          0.          0.          0.83924766  0.42857143\n",
      "  0.        ]\n",
      "[ 1.00000006  1.          0.          0.          0.83744969  0.42857143\n",
      "  0.        ]\n",
      "[ 1.00000002  1.          0.          0.          0.91381933  0.42857143\n",
      "  0.        ]\n",
      "[ 1.00000005  1.          0.          0.          0.89427083  0.57142857\n",
      "  0.        ]\n",
      "[ 0.38609249  1.          0.          0.          0.68850578  0.42857143\n",
      "  0.        ]\n"
     ]
    }
   ],
   "source": [
    "for fl in formatted_list:\n",
    "    print(fl.get_attribute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "### hyperparameters :\n",
    "\n",
    "1. learning_rate\n",
    "2. input_size\n",
    "3. rnn_output_size, cnn_output_size\n",
    "4. batch_size\n",
    "5. optimizer\n",
    "6. loss function\n",
    "7. n_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "input_size = 100  # word2vec k size\n",
    "batch_size = 100\n",
    "n_epochs = 100\n",
    "\n",
    "model = classifierModule(input_size, batch_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# target이 0일 때, p가 1-s보다 작으면 +1\n",
    "# target이 1일 때, p가 1-s보다 크면 +1\n",
    "# -> (1-s-p)*(t-1/2) <= 0 일 때 +1\n",
    "def get_accuracy(outputs, targets, sensitivity):\n",
    "    result = 0\n",
    "    t = targets.data-0.5\n",
    "    x = (1-sensitivity-outputs.data[:, 1])*t\n",
    "    for y in x:\n",
    "        if y <= 0:\n",
    "            result+=1\n",
    "    return result\n",
    "    \n",
    "def get_targets(batch):\n",
    "    targets = list()\n",
    "    for formatted in batch:\n",
    "        if formatted.label:\n",
    "            targets.append(1)\n",
    "        else:\n",
    "            targets.append(0)\n",
    "            \n",
    "    return Variable(torch.tensor(targets), requires_grad = False)\n",
    "\n",
    "def get_prediction(outputs, sensitivity):\n",
    "    return np.ceil(outputs.data[:, 1]+sensitivity-1+0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_net(self, train_list, validation_list, sensitivity = 0.5):\n",
    "    batch_list = test_classifier.resize_input(train_list)\n",
    "    \n",
    "    tacc_list = list()\n",
    "    vacc_list = list()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        tacc_list.append(0)\n",
    "        vacc_list.append(0)\n",
    "        \n",
    "        for bl in batch_list:\n",
    "            outputs = model(bl)\n",
    "            targets = get_targets(bl)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tacc_list[-1] += get_accuracy(outputs, targets, sensitivity)\n",
    "            \n",
    "            for input in bl:\n",
    "                reviewDB.add_spam_result(input.bookingReview.id, input.label)\n",
    "                \n",
    "            \n",
    "        tacc_list[-1] = tacc_list[-1] / len(train_list)\n",
    "        vacc_list[-1] = get_accuracy(model(validation_list), get_targets(validation_list), sensitivity) / len(validation_list)\n",
    "    \n",
    "        print(\"epoch {}: loss.data[0] {:.3f}|  train acc {:.3f}|  validation acc {:.3f}\" \n",
    "              .format(epoch, loss.data[0], tacc_list[-1], vacc_list[-1]))\n",
    "        \n",
    "        if epoch > 5 and np.mean(np.array(tacc_list[-6:-1])) < np.mean(np.array(vacc_list[-6:-1])):\n",
    "            print(\"Seems like m1 starts to overfit, aborting training\")\n",
    "            break\n",
    "            \n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ㄹ', 'ㄷ', 'ㄴ', 'ㅁ', 'ㄱ']\n",
      "['ㄱ', 'ㅁ', 'ㄴ', 'ㄷ', 'ㄹ']\n"
     ]
    }
   ],
   "source": [
    "a = [\"ㄱ\", \"ㄴ\", \"ㄷ\", \"ㄹ\", \"ㅁ\"]\n",
    "b = [4, 3, 2, 1, 0]\n",
    "c = [4, 2, 1, 0, 3]\n",
    "\n",
    "d = [x for _,x in sorted(zip(c,a))]\n",
    "print(d)\n",
    "list.reverse(d)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [4, 5, 6], [7, 8, 9], [10]]\n"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "a_ = list()\n",
    "for i in range(0, len(a), 3):\n",
    "    a_.append(a[i:i+3])\n",
    "print(a_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
