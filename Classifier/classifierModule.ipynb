{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io, os, sys, types\n",
    "from IPython import get_ipython\n",
    "from nbformat import read\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "def find_notebook(fullname, path=None):\n",
    "    \"\"\"find a notebook, given its fully qualified name and an optional path\n",
    "\n",
    "    This turns \"foo.bar\" into \"foo/bar.ipynb\"\n",
    "    and tries turning \"Foo_Bar\" into \"Foo Bar\" if Foo_Bar\n",
    "    does not exist.\n",
    "    \"\"\"\n",
    "    name = fullname.rsplit('.', 1)[-1]\n",
    "    if not path:\n",
    "        path = ['']\n",
    "    for d in path:\n",
    "        nb_path = os.path.join(d, name + \".ipynb\")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        # let import Notebook_Name find \"Notebook Name.ipynb\"\n",
    "        nb_path = nb_path.replace(\"_\", \" \")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        \n",
    "class NotebookLoader(object):\n",
    "    \"\"\"Module Loader for Jupyter Notebooks\"\"\"\n",
    "    def __init__(self, path=None):\n",
    "        self.shell = InteractiveShell.instance()\n",
    "        self.path = path\n",
    "\n",
    "    def load_module(self, fullname):\n",
    "        \"\"\"import a notebook as a module\"\"\"\n",
    "        path = find_notebook(fullname, self.path)\n",
    "\n",
    "        print (\"importing Jupyter notebook from %s\" % path)\n",
    "\n",
    "        # load the notebook object\n",
    "        with io.open(path, 'r', encoding='utf-8') as f:\n",
    "            nb = read(f, 4)\n",
    "\n",
    "\n",
    "        # create the module and add it to sys.modules\n",
    "        # if name in sys.modules:\n",
    "        #    return sys.modules[name]\n",
    "        mod = types.ModuleType(fullname)\n",
    "        mod.__file__ = path\n",
    "        mod.__loader__ = self\n",
    "        mod.__dict__['get_ipython'] = get_ipython\n",
    "        sys.modules[fullname] = mod\n",
    "\n",
    "        # extra work to ensure that magics that would affect the user_ns\n",
    "        # actually affect the notebook module's ns\n",
    "        save_user_ns = self.shell.user_ns\n",
    "        self.shell.user_ns = mod.__dict__\n",
    "\n",
    "        try:\n",
    "          for cell in nb.cells:\n",
    "            if cell.cell_type == 'code':\n",
    "                # transform the input to executable Python\n",
    "                code = self.shell.input_transformer_manager.transform_cell(cell.source)\n",
    "                # run the code in themodule\n",
    "                exec(code, mod.__dict__)\n",
    "        finally:\n",
    "            self.shell.user_ns = save_user_ns\n",
    "        return mod\n",
    "    \n",
    "class NotebookFinder(object):\n",
    "    \"\"\"Module finder that locates Jupyter Notebooks\"\"\"\n",
    "    def __init__(self):\n",
    "        self.loaders = {}\n",
    "\n",
    "    def find_module(self, fullname, path=None):\n",
    "        nb_path = find_notebook(fullname, path)\n",
    "        if not nb_path:\n",
    "            return\n",
    "\n",
    "        key = path\n",
    "        if path:\n",
    "            # lists aren't hashable\n",
    "            key = os.path.sep.join(path)\n",
    "\n",
    "        if key not in self.loaders:\n",
    "            self.loaders[key] = NotebookLoader(path)\n",
    "        return self.loaders[key]\n",
    "    \n",
    "sys.meta_path.append(NotebookFinder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from rnn.ipynb\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable   \n",
    "from torch import optim\n",
    "\n",
    "sys.path.append(\"../Preprocessor\")\n",
    "import format_module\n",
    "\n",
    "import rnn\n",
    "import naivebayesian\n",
    "import cnn\n",
    "import conclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_object(filename):\n",
    "    with open(filename, 'rb') as input:\n",
    "        return pickle.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class classifierModule(nn.Module):\n",
    "    def __init__(self, input_size, batch_size):\n",
    "        super(classifierModule, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.rnn_model = rnn.RNN_model(input_size)\n",
    "        self.rnn_out_size = rnn.RNN_model.hidden_size\n",
    "        #self.rnn_mlp()\n",
    "        \n",
    "        self.nb_model = naivebayesian.NaiveBayesianDB()\n",
    "        \n",
    "        self.cnn_model = cnn.ConvNet(input_size)\n",
    "        self.cnn_out_size = cnn.ConvNet.output_vector_size\n",
    "        #self.cnn_model = \n",
    "        \n",
    "        #TODO\n",
    "        self.conclude = conclude.conclude()\n",
    "        \n",
    "    def encoder(self, formattedList):\n",
    "        length = len(formattedList)\n",
    "        contextList = [formattedList[i].context for i in range(length)]\n",
    "\n",
    "        lengths = torch.LongTensor([len(contextList[i]) for i in range(length)])\n",
    "        max_len = torch.max(lengths)\n",
    "        \n",
    "        data = np.zeros((length, max_len, self.input_size))\n",
    "\n",
    "        for i in range(length):\n",
    "            context = contextList[i]\n",
    "            if not (context.size == 0):\n",
    "                data[i, :context.shape[0],:] = context\n",
    "            else:\n",
    "                lengths[i] = 1\n",
    "            \n",
    "        return self.sort_batch(torch.FloatTensor(data), formattedList, lengths)\n",
    "        \n",
    "    def sort_batch(self, context, formatted, seq_len):\n",
    "        batch_size = context.size(0)\n",
    "        sorted_seq_len, sorted_idx = seq_len.sort(0, descending = True)\n",
    "        \n",
    "        sorted_context = context[sorted_idx]\n",
    "        sorted_formatted = [formatted[i] for i in sorted_idx]\n",
    "\n",
    "        for f in sorted_formatted:\n",
    "            print(len(f.context))\n",
    "        \n",
    "        return Variable(sorted_context), sorted_formatted, sorted_seq_len\n",
    "    \n",
    "    def resize_input(self, input):\n",
    "        list_ = list()\n",
    "        for i in range(0, len(input), self.batch_size):\n",
    "            list_.append(input[i:i+self.batch_size])\n",
    "        return list_\n",
    "        \n",
    "    def forward(self, formatted_list, hidden=None):\n",
    "        context, formatted, lengths = self.encoder(bl)\n",
    "        \n",
    "        rnn_out = self.rnn_model(context, lengths)\n",
    "        cnn_out = self.cnn_model(context)\n",
    "        nb_out = self.nb_model.naive_bayes_FRlist(formatted)\n",
    "                \n",
    "        print(\"rnn_out : \", rnn_out, \"\\n\\n\\n\")\n",
    "        print(\"cnn_out : \", cnn_out, \"\\n\\n\\n\")\n",
    "        print(\"nb_out : \", nb_out, \"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137\n",
      "120\n",
      "52\n",
      "42\n",
      "34\n",
      "32\n",
      "27\n",
      "25\n",
      "20\n",
      "9\n",
      "Variable containing:\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "[torch.FloatTensor of size 10x1]\n",
      "\n",
      "rnn_out :  Variable containing:\n",
      " 0.0076  0.1047  0.1314  0.1635  0.0278 -0.0444  0.1449 -0.0566  0.1294  0.0668\n",
      " 0.0044  0.0744  0.1177  0.1710  0.0391 -0.0255  0.1544 -0.0455  0.1296  0.0638\n",
      " 0.0164  0.0910  0.1476  0.1858  0.0551 -0.0334  0.1668 -0.0322  0.1168  0.0588\n",
      " 0.0150  0.0828  0.1101  0.1768  0.0492 -0.0382  0.1486 -0.0435  0.1271  0.0639\n",
      "-0.0008  0.0855  0.1442  0.1828  0.0312 -0.0221  0.1565 -0.0523  0.1267  0.0545\n",
      " 0.0136  0.0950  0.1121  0.1792  0.0612 -0.0364  0.1575 -0.0372  0.1208  0.0553\n",
      " 0.0087  0.0871  0.1423  0.1769  0.0538 -0.0159  0.1597 -0.0590  0.1070  0.0442\n",
      " 0.0145  0.0832  0.1318  0.1786  0.0460 -0.0339  0.1650 -0.0441  0.1303  0.0624\n",
      "-0.0057  0.0883  0.1480  0.1953  0.0217 -0.0153  0.1861 -0.0611  0.1318  0.0601\n",
      " 0.0118  0.0750  0.1330  0.1776  0.0457 -0.0376  0.1640 -0.0322  0.1348  0.0617\n",
      "[torch.FloatTensor of size 10x10]\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "cnn_out :  Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.8472  0.6901  0.7524  0.7087  0.7292  0.9082  0.7914  0.7708  0.8572  0.4413\n",
      " 0.9049  0.6188  0.8787  0.8675  0.7925  0.5241  0.7983  0.9453  0.9034  0.7694\n",
      " 0.8425  0.9827  0.7351  0.9251  0.8716  0.9960  0.9256  0.9411  1.0108  0.8941\n",
      " 0.8649  0.8500  1.0466  0.7504  0.9499  0.7025  0.8133  0.8842  0.8677  0.7741\n",
      " 0.8314  0.6735  0.4880  0.9460  0.8170  0.5586  0.5629  0.6245  0.7440  0.8208\n",
      " 0.7602  0.8051  0.9018  0.7826  0.3604  0.7699  0.6097  0.8588  0.9852  0.9411\n",
      " 0.9515  0.6507  0.6927  0.7267  0.7974  0.4527  0.8488  0.5918  0.7627  0.3564\n",
      " 0.5539  0.5862  0.7661  0.6339  0.8595  0.9324  0.8895  0.8867  0.7143  0.8644\n",
      " 0.6920  0.8510  0.7757  0.7575  0.9840  0.8646  0.8704  0.9358  0.6418  0.7348\n",
      " 0.6819  0.6086  0.8304  0.9723  0.7259  0.7086  0.7086  0.7086  0.7086  0.7086\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.8060  0.5500  0.6735  0.9594  0.8710  0.8692  0.8620  0.9374  0.8457  0.8535\n",
      " 0.7863  0.8405  0.8820  0.7053  0.7199  0.8602  0.3317  0.6399  1.0222  0.9145\n",
      " 0.9504  0.6200  1.0187  0.9071  0.8747  0.9391  1.1548  0.9833  0.7635  0.8800\n",
      " 0.8889  0.9037  0.8282  0.8218  0.8587  0.7001  0.6462  0.8452  0.7148  0.7171\n",
      " 0.6995  1.0160  0.5186  0.7031  0.6879  0.7651  0.7086  0.7086  0.7086  0.7086\n",
      " 0.9768  0.7337  0.4398  0.6736  0.6655  0.7086  0.7086  0.7086  0.7086  0.7086\n",
      " 0.8202  0.8381  0.7890  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086\n",
      " 0.7541  0.8489  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086\n",
      " 0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086\n",
      " 0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.7698  0.9385  0.9050  1.0247  1.0241  0.8694  0.9247  1.0776  0.8788  0.8021\n",
      " 1.0331  0.7315  0.9536  0.7599  0.3641  0.9569  0.8666  0.8706  0.7748  0.9671\n",
      " 0.9283  0.6988  0.8873  0.7287  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086\n",
      " 0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086\n",
      " 0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086\n",
      " 0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086\n",
      " 0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086\n",
      " 0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086\n",
      " 0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086\n",
      " 0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086\n",
      "\n",
      "Columns 30 to 39 \n",
      " 0.7240  0.7746  0.7489  0.6624  0.7670  0.5734  0.6897  0.9314  0.6664  0.9809\n",
      " 0.9641  0.3964  0.8116  0.8530  0.8581  1.0385  0.6010  0.5163  0.8077  0.6165\n",
      " 0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086\n",
      " 0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086\n",
      " 0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086\n",
      " 0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086\n",
      " 0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086\n",
      " 0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086\n",
      " 0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086\n",
      " 0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086\n",
      "\n",
      "Columns 40 to 49 \n",
      " 0.6909  0.7189  0.6490  0.7607  0.7326  0.8539  0.8764  0.8031  0.8067  0.6765\n",
      " 0.6751  0.7760  0.5057  0.6810  0.9455  0.7731  0.6406  0.9027  0.6070  1.0999\n",
      " 0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086\n",
      " 0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086\n",
      " 0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086\n",
      " 0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086\n",
      " 0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086\n",
      " 0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086\n",
      " 0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086\n",
      " 0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086\n",
      "\n",
      "Columns 50 to 59 \n",
      " 0.8359  0.6916  0.8663  0.6010  0.9618  0.7790  0.9548  0.6534  0.8025  0.9453\n",
      " 0.6540  0.2615  0.6710  0.6515  0.7086  0.7086  0.7086  0.7086  0.7086  0.7158\n",
      " 0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7158\n",
      " 0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7158\n",
      " 0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7158\n",
      " 0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7158\n",
      " 0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7158\n",
      " 0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7158\n",
      " 0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7158\n",
      " 0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7086  0.7158\n",
      "[torch.FloatTensor of size 10x60]\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "nb_out :  Variable containing:\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "[torch.FloatTensor of size 10x1]\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "141\n",
      "124\n",
      "104\n",
      "71\n",
      "63\n",
      "33\n",
      "27\n",
      "25\n",
      "19\n",
      "7\n",
      "Variable containing:\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "[torch.FloatTensor of size 10x1]\n",
      "\n",
      "rnn_out :  Variable containing:\n",
      " 0.0094  0.0900  0.1126  0.1561  0.0496 -0.0382  0.1265 -0.0462  0.1303  0.0693\n",
      " 0.0104  0.0973  0.1065  0.1631  0.0547 -0.0178  0.1501 -0.0623  0.1096  0.0529\n",
      "-0.0061  0.0920  0.1620  0.1903  0.0164 -0.0391  0.1588 -0.0385  0.1372  0.0693\n",
      " 0.0203  0.0849  0.1104  0.1703  0.0665 -0.0291  0.1551 -0.0550  0.1231  0.0444\n",
      " 0.0139  0.0860  0.1199  0.1718  0.0493 -0.0355  0.1496 -0.0443  0.1318  0.0668\n",
      "-0.0025  0.0801  0.1502  0.1786  0.0351 -0.0103  0.1673 -0.0556  0.1302  0.0728\n",
      " 0.0138  0.0822  0.1255  0.1845  0.0400 -0.0347  0.1638 -0.0532  0.1335  0.0610\n",
      " 0.0024  0.0861  0.1252  0.1812  0.0376 -0.0420  0.1560 -0.0295  0.1320  0.0699\n",
      " 0.0170  0.0927  0.1209  0.1738  0.0629 -0.0429  0.1547 -0.0382  0.1260  0.0578\n",
      " 0.0041  0.1002  0.1170  0.1796  0.0495 -0.0451  0.1555 -0.0200  0.1238  0.0595\n",
      "[torch.FloatTensor of size 10x10]\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "cnn_out :  Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.7939  0.8492  0.8113  0.9196  1.0128  0.7711  0.7095  0.6428  0.9838  0.6227\n",
      " 1.0526  0.8039  0.8644  0.6732  0.6649  0.6152  1.0505  0.7383  0.9312  0.9673\n",
      " 0.7262  0.7430  0.9365  0.8380  0.9152  1.0135  0.7039  0.7910  0.6101  1.0757\n",
      " 0.8479  0.8640  0.9477  0.7829  0.8422  0.8241  0.6527  0.9406  0.8302  1.0848\n",
      " 0.6403  0.7357  0.7956  0.7358  0.9005  0.7298  0.7376  0.7812  0.9731  0.8772\n",
      " 0.7421  0.9412  0.8464  0.5726  0.7502  0.8704  0.7907  0.9539  0.8530  1.0152\n",
      " 0.6751  0.6530  0.7928  1.0666  0.8415  0.5667  0.7955  0.5313  0.6443  0.8591\n",
      " 0.8141  0.8483  0.6480  0.8926  0.8234  0.3227  0.9724  0.8880  0.9689  0.6561\n",
      " 0.8584  0.7955  0.9726  0.9617  1.0857  0.7653  0.8337  0.8089  0.7612  0.7286\n",
      " 0.8124  0.8754  0.6234  0.8174  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.8230  0.9208  0.8237  1.0724  0.8898  0.7667  0.9289  0.7470  1.0010  1.0098\n",
      " 1.0904  0.7937  0.8473  0.7445  0.6897  1.0341  0.8618  0.7478  0.7529  1.0184\n",
      " 0.7100  0.9289  0.4777  0.5137  0.5898  0.6711  0.6208  0.8321  0.5302  0.8698\n",
      " 0.5306  0.7090  1.1126  1.0557  0.7967  0.7968  0.4402  1.0451  0.9868  0.6936\n",
      " 0.8215  0.7441  0.8663  0.6454  0.8716  1.0208  0.4544  0.9693  0.9209  0.9145\n",
      " 0.7912  0.9338  0.9876  0.6981  0.6864  0.7286  0.7286  0.7286  0.7286  0.7286\n",
      " 0.9063  0.8785  0.7218  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286\n",
      " 0.6461  0.8167  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286\n",
      " 0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286\n",
      " 0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.7863  0.9407  0.8048  0.9272  0.7425  0.7839  0.7194  0.7550  0.7362  0.6525\n",
      " 0.9275  0.9171  0.7791  0.7061  0.8710  0.7690  0.7089  0.7974  0.8796  0.8326\n",
      " 0.7418  1.0848  0.7517  0.7544  0.9157  0.8398  0.7842  0.9213  0.7703  0.8249\n",
      " 0.8172  0.9069  0.7742  0.8664  0.9450  0.8044  0.7485  0.6515  0.7863  0.9096\n",
      " 0.8236  0.8431  0.8903  0.8056  0.8879  1.0150  0.7612  0.7629  0.7286  0.7286\n",
      " 0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286\n",
      " 0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286\n",
      " 0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286\n",
      " 0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286\n",
      " 0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286\n",
      "\n",
      "Columns 30 to 39 \n",
      " 0.6621  0.8290  0.7615  0.6264  0.7474  0.7644  0.6901  0.8749  0.7423  0.7137\n",
      " 1.0581  0.9117  0.8588  0.9053  0.9882  0.8886  0.8013  0.8879  0.9903  0.7194\n",
      " 0.7538  0.6225  0.8710  1.0687  0.8004  0.8390  0.7716  0.7291  0.6852  0.8605\n",
      " 0.8554  0.7317  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286\n",
      " 0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286\n",
      " 0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286\n",
      " 0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286\n",
      " 0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286\n",
      " 0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286\n",
      " 0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286\n",
      "\n",
      "Columns 40 to 49 \n",
      " 0.9441  0.9323  0.8223  0.5113  0.6883  0.5826  0.6059  0.7997  0.7996  0.6153\n",
      " 0.6403  0.7828  1.0363  0.7325  0.7851  0.8936  0.5722  0.9482  0.4313  0.5550\n",
      " 0.7655  0.7403  0.8298  0.9095  0.6524  0.7245  0.7286  0.7286  0.7286  0.7286\n",
      " 0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286\n",
      " 0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286\n",
      " 0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286\n",
      " 0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286\n",
      " 0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286\n",
      " 0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286\n",
      " 0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286\n",
      "\n",
      "Columns 50 to 59 \n",
      " 0.4969  0.8781  0.8958  0.7828  0.8776  0.8081  0.9644  0.8018  0.7059  0.9586\n",
      " 0.6736  0.7892  0.7243  0.7608  0.7286  0.7286  0.7286  0.7286  0.7286  0.7337\n",
      " 0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7337\n",
      " 0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7337\n",
      " 0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7337\n",
      " 0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7337\n",
      " 0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7337\n",
      " 0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7337\n",
      " 0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7337\n",
      " 0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7286  0.7337\n",
      "[torch.FloatTensor of size 10x60]\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "nb_out :  Variable containing:\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "[torch.FloatTensor of size 10x1]\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "112\n",
      "85\n",
      "83\n",
      "54\n",
      "45\n",
      "28\n",
      "22\n",
      "20\n",
      "13\n",
      "10\n",
      "Variable containing:\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "[torch.FloatTensor of size 10x1]\n",
      "\n",
      "rnn_out :  Variable containing:\n",
      " 0.0128  0.0853  0.1152  0.1842  0.0580 -0.0351  0.1585 -0.0466  0.1257  0.0461\n",
      " 0.0125  0.0872  0.1129  0.1776  0.0587 -0.0217  0.1566 -0.0475  0.1182  0.0501\n",
      " 0.0019  0.0773  0.1671  0.1857  0.0346 -0.0149  0.1669 -0.0600  0.1274  0.0443\n",
      " 0.0004  0.0948  0.1352  0.1842  0.0354 -0.0270  0.1621 -0.0420  0.1217  0.0623\n",
      " 0.0195  0.0913  0.0943  0.1668  0.0567 -0.0399  0.1502 -0.0452  0.1213  0.0682\n",
      " 0.0044  0.0847  0.1189  0.1802  0.0304 -0.0293  0.1683 -0.0530  0.1230  0.0577\n",
      " 0.0042  0.0835  0.1262  0.1816  0.0378 -0.0367  0.1465 -0.0358  0.1338  0.0652\n",
      "-0.0002  0.0835  0.1507  0.1834  0.0463 -0.0259  0.1655 -0.0398  0.1137  0.0506\n",
      " 0.0030  0.0761  0.1709  0.1907  0.0253 -0.0292  0.1549 -0.0485  0.1438  0.0634\n",
      "-0.0180  0.0739  0.1643  0.1791  0.0114 -0.0103  0.1566 -0.0589  0.1494  0.0584\n",
      "[torch.FloatTensor of size 10x10]\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "cnn_out :  Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.5160  0.6464  0.6119  0.4370  0.5929  0.8325  0.8861  0.7535  0.5860  0.6418\n",
      " 0.6141  0.5341  0.8309  0.6804  0.7034  0.5158  0.5738  0.8078  0.7572  0.4977\n",
      " 0.6955  0.6856  0.6516  0.6241  0.6984  0.7232  0.8361  0.5523  0.7457  0.7840\n",
      " 0.6604  0.7009  0.6957  0.6342  0.7571  0.5371  0.5710  0.5443  0.6293  0.7249\n",
      " 0.6297  0.5326  0.5934  0.5354  0.6888  0.5061  0.6095  0.5584  0.7519  0.6965\n",
      " 0.8167  0.7532  0.5161  0.6470  0.5159  0.5348  0.7272  0.6346  0.6053  0.7625\n",
      " 0.5767  0.5495  0.7599  0.6705  0.7360  0.5516  0.4858  0.4592  0.4563  0.5067\n",
      " 0.5529  0.6877  0.7865  0.7446  0.6469  0.5436  0.6185  0.8495  0.4342  0.4806\n",
      " 0.5674  0.6323  0.7266  0.4893  0.6200  0.5955  0.4859  0.5748  0.5757  0.5753\n",
      " 0.5193  0.6792  0.5969  0.6360  0.5559  0.6098  0.5705  0.5753  0.5753  0.5753\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.8102  0.5846  0.8382  0.7465  0.6692  0.6539  0.7751  0.7219  0.5553  0.6204\n",
      " 0.4171  0.6504  0.6470  0.5839  0.8396  0.9061  0.8400  0.6044  0.7677  0.7056\n",
      " 0.6838  0.7124  0.6094  0.7243  0.4929  0.5136  0.7762  0.7653  0.6165  0.6177\n",
      " 0.7787  0.6425  0.5025  0.4941  0.6733  0.6965  0.5739  0.6024  0.6952  0.6972\n",
      " 0.7607  0.4487  0.6623  0.8111  0.6435  0.7445  0.6277  0.5913  0.5828  0.6392\n",
      " 0.6740  0.5510  0.7003  0.6784  0.5031  0.4609  0.5711  0.5753  0.5753  0.5753\n",
      " 0.7149  0.6134  0.5222  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753\n",
      " 0.6203  0.5968  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753\n",
      " 0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753\n",
      " 0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.6160  0.5505  0.6908  0.6265  0.7196  0.3754  0.3968  0.8252  0.3849  0.5719\n",
      " 0.7848  0.6562  0.7777  0.5464  0.5305  0.6427  0.5033  0.4609  0.5523  0.5346\n",
      " 0.6041  0.7326  0.7689  0.7699  0.6305  0.4617  0.3331  0.6048  0.7421  0.7802\n",
      " 0.5205  0.8481  0.7040  0.7167  0.6484  0.6318  0.6676  0.6080  0.5594  0.6151\n",
      " 0.6331  0.7495  0.6879  0.7692  0.6969  0.5794  0.5753  0.5753  0.5753  0.5753\n",
      " 0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753\n",
      " 0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753\n",
      " 0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753\n",
      " 0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753\n",
      " 0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753\n",
      "\n",
      "Columns 30 to 39 \n",
      " 0.5035  0.6862  0.4194  0.6619  0.7773  0.6645  0.5940  0.6509  0.5431  0.5054\n",
      " 0.5980  0.5336  0.6815  0.8126  0.5304  0.6723  0.6420  0.5083  0.5724  0.4970\n",
      " 0.4980  0.5499  0.7161  0.6136  0.6214  0.6693  0.8039  0.5675  0.7272  0.6878\n",
      " 0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753\n",
      " 0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753\n",
      " 0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753\n",
      " 0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753\n",
      " 0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753\n",
      " 0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753\n",
      " 0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753\n",
      "\n",
      "Columns 40 to 49 \n",
      " 0.6254  0.5513  0.8438  0.8893  0.8431  0.6762  0.8363  0.5915  0.7311  0.8356\n",
      " 0.5823  0.6495  0.5613  0.5700  0.5837  0.5720  0.5798  0.5753  0.5753  0.5753\n",
      " 0.5822  0.8625  0.5459  0.4792  0.8841  0.5876  0.5753  0.5753  0.5753  0.5753\n",
      " 0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753\n",
      " 0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753\n",
      " 0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753\n",
      " 0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753\n",
      " 0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753\n",
      " 0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753\n",
      " 0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753\n",
      "\n",
      "Columns 50 to 59 \n",
      " 0.7161  0.5359  0.7086  0.4789  0.5996  0.5827  0.6244  0.7285  0.6432  0.7574\n",
      " 0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5843\n",
      " 0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5843\n",
      " 0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5843\n",
      " 0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5843\n",
      " 0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5843\n",
      " 0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5843\n",
      " 0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5843\n",
      " 0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5843\n",
      " 0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5753  0.5843\n",
      "[torch.FloatTensor of size 10x60]\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "nb_out :  Variable containing:\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "[torch.FloatTensor of size 10x1]\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "86\n",
      "47\n",
      "46\n",
      "33\n",
      "27\n",
      "25\n",
      "15\n",
      "13\n",
      "8\n",
      "6\n",
      "Variable containing:\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "[torch.FloatTensor of size 10x1]\n",
      "\n",
      "rnn_out :  Variable containing:\n",
      " 0.0051  0.0816  0.1313  0.1722  0.0480 -0.0267  0.1548 -0.0543  0.1305  0.0539\n",
      " 0.0098  0.0941  0.1219  0.1724  0.0542 -0.0229  0.1608 -0.0469  0.1142  0.0512\n",
      "-0.0065  0.0669  0.1905  0.1873  0.0205 -0.0081  0.1674 -0.0741  0.1416  0.0438\n",
      "-0.0083  0.0862  0.1339  0.1706  0.0354 -0.0204  0.1409 -0.0448  0.1258  0.0691\n",
      " 0.0012  0.0731  0.1289  0.1844  0.0318 -0.0253  0.1760 -0.0500  0.1335  0.0462\n",
      " 0.0137  0.0892  0.1286  0.1880  0.0581 -0.0322  0.1588 -0.0491  0.1185  0.0407\n",
      "-0.0074  0.0930  0.1438  0.1833  0.0357 -0.0288  0.1488 -0.0376  0.1252  0.0676\n",
      " 0.0048  0.0650  0.1103  0.1878  0.0501 -0.0128  0.1580 -0.0429  0.1259  0.0525\n",
      " 0.0097  0.0806  0.1383  0.1671  0.0408 -0.0336  0.1546 -0.0518  0.1376  0.0630\n",
      " 0.0054  0.0748  0.1247  0.1677  0.0431 -0.0339  0.1416 -0.0368  0.1322  0.0701\n",
      "[torch.FloatTensor of size 10x10]\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "cnn_out :  Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.5477  0.5563  0.7369  0.5920  0.5976  0.5578  0.3987  0.4704  0.5545  0.6128\n",
      " 0.4641  0.5127  0.5604  0.5341  0.3424  0.5916  0.5176  0.6319  0.5397  0.5845\n",
      " 0.4460  0.3520  0.4546  0.4867  0.4927  0.3803  0.4419  0.5028  0.4569  0.5538\n",
      " 0.4306  0.5331  0.5082  0.3741  0.3254  0.6276  0.6092  0.3075  0.5948  0.4914\n",
      " 0.4389  0.4184  0.5502  0.4945  0.3829  0.4709  0.3795  0.4736  0.4934  0.4402\n",
      " 0.4238  0.4867  0.5139  0.6171  0.6590  0.4311  0.3495  0.3352  0.6576  0.6747\n",
      " 0.5923  0.5595  0.3957  0.4981  0.5421  0.6187  0.5767  0.3107  0.4942  0.6082\n",
      " 0.5631  0.5787  0.4538  0.4824  0.5769  0.4466  0.4873  0.5514  0.4607  0.4796\n",
      " 0.4681  0.3270  0.4589  0.4784  0.5309  0.5115  0.4297  0.4480  0.4480  0.4480\n",
      " 0.6019  0.2950  0.4867  0.6473  0.4973  0.4622  0.4480  0.4480  0.4480  0.4480\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.4573  0.4237  0.3329  0.6153  0.4719  0.5283  0.6297  0.6098  0.5508  0.3243\n",
      " 0.4691  0.4682  0.6156  0.4947  0.5317  0.6140  0.4267  0.3985  0.4061  0.5003\n",
      " 0.5575  0.5994  0.4104  0.4136  0.4511  0.4954  0.5761  0.5355  0.4799  0.4510\n",
      " 0.2563  0.3731  0.5244  0.5577  0.4561  0.4696  0.5414  0.5093  0.5469  0.6433\n",
      " 0.4305  0.5367  0.5725  0.3707  0.4135  0.5892  0.5502  0.5499  0.5514  0.4654\n",
      " 0.6186  0.6101  0.4943  0.3885  0.4517  0.3747  0.5540  0.4951  0.4638  0.4480\n",
      " 0.4258  0.4376  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480\n",
      " 0.4736  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480\n",
      " 0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480\n",
      " 0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.6049  0.4624  0.3772  0.5981  0.5071  0.3523  0.3830  0.4611  0.6019  0.7050\n",
      " 0.5148  0.4675  0.4585  0.4501  0.4391  0.5749  0.4234  0.5922  0.6311  0.3856\n",
      " 0.3585  0.3841  0.6178  0.6967  0.6048  0.3908  0.6599  0.4581  0.4206  0.4619\n",
      " 0.5069  0.3818  0.2696  0.4079  0.4444  0.4480  0.4480  0.4480  0.4480  0.4480\n",
      " 0.4518  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480\n",
      " 0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480\n",
      " 0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480\n",
      " 0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480\n",
      " 0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480\n",
      " 0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480\n",
      "\n",
      "Columns 30 to 39 \n",
      " 0.5442  0.3103  0.3758  0.6450  0.5449  0.5299  0.5892  0.3791  0.5567  0.6804\n",
      " 0.4035  0.5464  0.4494  0.4466  0.4516  0.4480  0.4480  0.4480  0.4480  0.4480\n",
      " 0.3531  0.3772  0.4743  0.4551  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480\n",
      " 0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480\n",
      " 0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480\n",
      " 0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480\n",
      " 0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480\n",
      " 0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480\n",
      " 0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480\n",
      " 0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480\n",
      "\n",
      "Columns 40 to 49 \n",
      " 0.5504  0.2983  0.5561  0.4991  0.5674  0.3550  0.5173  0.5878  0.5129  0.1884\n",
      " 0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480\n",
      " 0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480\n",
      " 0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480\n",
      " 0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480\n",
      " 0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480\n",
      " 0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480\n",
      " 0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480\n",
      " 0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480\n",
      " 0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480\n",
      "\n",
      "Columns 50 to 59 \n",
      " 0.4986  0.4733  0.4264  0.5858  0.4699  0.4950  0.4657  0.5751  0.3840  0.3443\n",
      " 0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4533\n",
      " 0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4533\n",
      " 0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4533\n",
      " 0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4533\n",
      " 0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4533\n",
      " 0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4533\n",
      " 0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4533\n",
      " 0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4533\n",
      " 0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4480  0.4533\n",
      "[torch.FloatTensor of size 10x60]\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "nb_out :  Variable containing:\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "[torch.FloatTensor of size 10x1]\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "96\n",
      "73\n",
      "68\n",
      "49\n",
      "41\n",
      "28\n",
      "16\n",
      "15\n",
      "15\n",
      "7\n",
      "Variable containing:\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "[torch.FloatTensor of size 10x1]\n",
      "\n",
      "rnn_out :  Variable containing:\n",
      "-0.0023  0.0809  0.1480  0.1813  0.0291 -0.0345  0.1540 -0.0434  0.1470  0.0739\n",
      " 0.0045  0.0674  0.1553  0.1819  0.0338 -0.0176  0.1679 -0.0565  0.1370  0.0554\n",
      " 0.0138  0.0962  0.1262  0.1732  0.0605 -0.0320  0.1521 -0.0450  0.1144  0.0564\n",
      "-0.0025  0.0769  0.1541  0.1825  0.0429 -0.0089  0.1693 -0.0484  0.1211  0.0454\n",
      " 0.0106  0.0857  0.1227  0.1764  0.0409 -0.0523  0.1506 -0.0298  0.1310  0.0700\n",
      "-0.0003  0.0877  0.1617  0.1887  0.0437 -0.0125  0.1727 -0.0580  0.1151  0.0371\n",
      " 0.0092  0.0840  0.1525  0.1877  0.0493 -0.0211  0.1643 -0.0440  0.1191  0.0479\n",
      "-0.0015  0.0760  0.1747  0.1787  0.0363 -0.0019  0.1660 -0.0643  0.1269  0.0482\n",
      " 0.0056  0.0918  0.1403  0.1780  0.0504 -0.0355  0.1519 -0.0413  0.1292  0.0565\n",
      " 0.0059  0.0806  0.1576  0.1664  0.0347 -0.0333  0.1594 -0.0476  0.1370  0.0576\n",
      "[torch.FloatTensor of size 10x10]\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "cnn_out :  Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.5278  0.5258  0.6061  0.4809  0.3226  0.4269  0.5898  0.6347  0.3741  0.2576\n",
      " 0.6378  0.4078  0.6238  0.7551  0.5463  0.4269  0.4675  0.4251  0.4838  0.5320\n",
      " 0.5524  0.6908  0.6692  0.5277  0.5717  0.4839  0.6137  0.5225  0.4202  0.5093\n",
      " 0.5450  0.5486  0.7140  0.5471  0.4983  0.4588  0.7097  0.4476  0.5893  0.7219\n",
      " 0.5644  0.4380  0.6514  0.6482  0.4928  0.4052  0.5935  0.4514  0.5280  0.5283\n",
      " 0.6137  0.6158  0.5435  0.6199  0.4426  0.5792  0.5653  0.6151  0.6227  0.4434\n",
      " 0.5648  0.5034  0.6692  0.6245  0.4502  0.7546  0.5268  0.4690  0.5857  0.4926\n",
      " 0.5337  0.4948  0.5669  0.5052  0.5241  0.6388  0.6101  0.2241  0.4567  0.6414\n",
      " 0.5834  0.5707  0.6631  0.5423  0.7041  0.5117  0.5706  0.5457  0.6437  0.5700\n",
      " 0.4949  0.5098  0.5351  0.4005  0.5229  0.4926  0.4931  0.4931  0.4931  0.4931\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.6441  0.4462  0.3934  0.7472  0.5022  0.4316  0.5620  0.7047  0.6693  0.5110\n",
      " 0.6703  0.5956  0.4334  0.6003  0.5969  0.7880  0.8095  0.6616  0.5614  0.4180\n",
      " 0.7030  0.6301  0.4081  0.6119  0.6800  0.6831  0.5481  0.5118  0.3935  0.4257\n",
      " 0.6170  0.4308  0.4009  0.5886  0.5848  0.6668  0.5937  0.4908  0.5017  0.4067\n",
      " 0.7070  0.6685  0.5009  0.6836  0.5455  0.6075  0.4902  0.5116  0.4994  0.7032\n",
      " 0.6149  0.7014  0.6618  0.6341  0.4798  0.2294  0.4644  0.7129  0.5284  0.4931\n",
      " 0.4950  0.4947  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931\n",
      " 0.4981  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931\n",
      " 0.5129  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931\n",
      " 0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.6269  0.5372  0.5140  0.5041  0.2336  0.5626  0.5997  0.5506  0.5046  0.6228\n",
      " 0.5227  0.4587  0.6848  0.3890  0.5100  0.7618  0.7517  0.5256  0.5297  0.6583\n",
      " 0.5311  0.5805  0.3901  0.4431  0.5729  0.5976  0.5317  0.5301  0.6273  0.4851\n",
      " 0.5900  0.6701  0.6377  0.7471  0.5290  0.6062  0.4729  0.4437  0.6057  0.5491\n",
      " 0.3494  0.5670  0.5615  0.5899  0.5598  0.5039  0.5217  0.4931  0.4931  0.4931\n",
      " 0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931\n",
      " 0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931\n",
      " 0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931\n",
      " 0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931\n",
      " 0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931\n",
      "\n",
      "Columns 30 to 39 \n",
      " 0.6481  0.4286  0.4898  0.4671  0.5109  0.5116  0.4448  0.6171  0.5744  0.5033\n",
      " 0.7164  0.6279  0.5245  0.5827  0.6017  0.6330  0.5114  0.4434  0.4941  0.6207\n",
      " 0.3806  0.5461  0.3452  0.4636  0.5043  0.4779  0.5265  0.5271  0.5251  0.6758\n",
      " 0.4236  0.5547  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931\n",
      " 0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931\n",
      " 0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931\n",
      " 0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931\n",
      " 0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931\n",
      " 0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931\n",
      " 0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931\n",
      "\n",
      "Columns 40 to 49 \n",
      " 0.6009  0.6404  0.4993  0.5258  0.5511  0.5272  0.4741  0.5957  0.5658  0.4951\n",
      " 0.4950  0.5978  0.5215  0.4326  0.6423  0.6260  0.5328  0.4931  0.4931  0.4931\n",
      " 0.5203  0.5856  0.4444  0.5018  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931\n",
      " 0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931\n",
      " 0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931\n",
      " 0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931\n",
      " 0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931\n",
      " 0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931\n",
      " 0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931\n",
      " 0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931\n",
      "\n",
      "Columns 50 to 59 \n",
      " 0.5355  0.6162  0.5130  0.6254  0.4788  0.3229  0.4053  0.6716  0.6726  0.4619\n",
      " 0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4985\n",
      " 0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4985\n",
      " 0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4985\n",
      " 0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4985\n",
      " 0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4985\n",
      " 0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4985\n",
      " 0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4985\n",
      " 0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4985\n",
      " 0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4931  0.4985\n",
      "[torch.FloatTensor of size 10x60]\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "nb_out :  Variable containing:\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "[torch.FloatTensor of size 10x1]\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "88\n",
      "83\n",
      "70\n",
      "55\n",
      "48\n",
      "43\n",
      "43\n",
      "27\n",
      "20\n",
      "12\n",
      "Variable containing:\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "[torch.FloatTensor of size 10x1]\n",
      "\n",
      "rnn_out :  Variable containing:\n",
      " 0.0095  0.0680  0.1433  0.1720  0.0445 -0.0198  0.1640 -0.0578  0.1370  0.0576\n",
      " 0.0109  0.0790  0.1338  0.1785  0.0396 -0.0339  0.1530 -0.0533  0.1376  0.0618\n",
      "-0.0157  0.0870  0.1804  0.1832  0.0088 -0.0214  0.1507 -0.0575  0.1447  0.0625\n",
      "-0.0094  0.0841  0.1503  0.1927  0.0186 -0.0444  0.1553 -0.0255  0.1416  0.0768\n",
      " 0.0087  0.0934  0.1190  0.1814  0.0396 -0.0459  0.1563 -0.0341  0.1321  0.0687\n",
      " 0.0056  0.0872  0.1396  0.1869  0.0470 -0.0326  0.1677 -0.0443  0.1250  0.0564\n",
      " 0.0060  0.0848  0.1283  0.1708  0.0466 -0.0219  0.1458 -0.0583  0.1341  0.0489\n",
      " 0.0016  0.0783  0.1467  0.1912  0.0196 -0.0329  0.1611 -0.0566  0.1451  0.0584\n",
      " 0.0034  0.0792  0.1351  0.1770  0.0416 -0.0367  0.1505 -0.0368  0.1449  0.0705\n",
      " 0.0026  0.0907  0.1339  0.1729  0.0354 -0.0395  0.1460 -0.0395  0.1350  0.0705\n",
      "[torch.FloatTensor of size 10x10]\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "cnn_out :  Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.4883  0.3934  0.5689  0.5840  0.4723  0.4399  0.4752  0.6009  0.4276  0.4561\n",
      " 0.5505  0.4748  0.5895  0.5963  0.5343  0.4938  0.6194  0.5292  0.4460  0.4630\n",
      " 0.4960  0.4815  0.4523  0.4687  0.5010  0.3992  0.4403  0.6141  0.5269  0.5000\n",
      " 0.4692  0.4831  0.4337  0.5562  0.4468  0.4645  0.6027  0.3990  0.3098  0.4310\n",
      " 0.4906  0.7036  0.4211  0.4464  0.5533  0.5214  0.5941  0.4504  0.5404  0.4236\n",
      " 0.5365  0.5305  0.5772  0.5346  0.5189  0.6058  0.5195  0.5000  0.5064  0.5701\n",
      " 0.4792  0.5382  0.6149  0.5627  0.4935  0.5654  0.5657  0.6693  0.4900  0.2600\n",
      " 0.5151  0.3985  0.5843  0.5370  0.5798  0.4980  0.6227  0.4476  0.3374  0.4657\n",
      " 0.6245  0.5168  0.4981  0.4718  0.4188  0.5644  0.4572  0.4291  0.3632  0.4746\n",
      " 0.4562  0.4317  0.4575  0.4661  0.4554  0.4461  0.5024  0.5912  0.4297  0.4813\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.5209  0.4443  0.5371  0.4686  0.6040  0.5070  0.4569  0.5731  0.5273  0.5337\n",
      " 0.4952  0.5368  0.4345  0.4094  0.5718  0.5401  0.3539  0.3110  0.5334  0.5165\n",
      " 0.4226  0.4338  0.4815  0.5289  0.5281  0.5741  0.5376  0.5130  0.5216  0.4699\n",
      " 0.5561  0.5222  0.4496  0.5406  0.4929  0.4332  0.5215  0.5806  0.4231  0.4511\n",
      " 0.4734  0.5383  0.4535  0.3550  0.4444  0.4591  0.5890  0.6544  0.5648  0.3609\n",
      " 0.3703  0.5498  0.5571  0.6072  0.4902  0.4933  0.5201  0.5146  0.5466  0.6338\n",
      " 0.3353  0.5417  0.3915  0.4916  0.3603  0.3736  0.4118  0.5245  0.4968  0.5557\n",
      " 0.5342  0.6346  0.5110  0.5302  0.5517  0.2951  0.5445  0.6596  0.4626  0.4599\n",
      " 0.4767  0.4845  0.4940  0.4884  0.4447  0.4507  0.4507  0.4507  0.4507  0.4507\n",
      " 0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.4839  0.4978  0.5808  0.5755  0.6254  0.4364  0.4296  0.4636  0.6592  0.6855\n",
      " 0.6077  0.4212  0.3709  0.4073  0.5795  0.4870  0.6448  0.5224  0.7135  0.5926\n",
      " 0.3165  0.4351  0.6103  0.5676  0.4270  0.5746  0.5738  0.6442  0.5646  0.5536\n",
      " 0.5049  0.4642  0.3628  0.4254  0.6006  0.5400  0.4346  0.4739  0.4353  0.5546\n",
      " 0.6567  0.6158  0.4763  0.4718  0.5489  0.5967  0.5618  0.3615  0.6440  0.5677\n",
      " 0.6349  0.5990  0.6829  0.5054  0.4931  0.5619  0.4322  0.4315  0.4737  0.4644\n",
      " 0.5123  0.4825  0.4589  0.5103  0.5344  0.6499  0.5543  0.4415  0.2132  0.3332\n",
      " 0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507\n",
      " 0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507\n",
      " 0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507\n",
      "\n",
      "Columns 30 to 39 \n",
      " 0.5955  0.5935  0.5914  0.5815  0.5154  0.4866  0.4465  0.6220  0.5213  0.4786\n",
      " 0.3052  0.3011  0.4412  0.4650  0.4486  0.1892  0.1744  0.4690  0.6731  0.5376\n",
      " 0.6297  0.4871  0.5200  0.5902  0.5126  0.4719  0.6224  0.5986  0.4721  0.4133\n",
      " 0.5868  0.5233  0.6426  0.6486  0.4345  0.5034  0.4072  0.4807  0.4736  0.4507\n",
      " 0.5382  0.4177  0.4193  0.5244  0.4608  0.4507  0.4507  0.4507  0.4507  0.4507\n",
      " 0.5264  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507\n",
      " 0.4717  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507\n",
      " 0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507\n",
      " 0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507\n",
      " 0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507\n",
      "\n",
      "Columns 40 to 49 \n",
      " 0.6913  0.5149  0.4631  0.3772  0.4358  0.5332  0.3982  0.4408  0.5744  0.6781\n",
      " 0.6060  0.3563  0.3909  0.5855  0.4003  0.4886  0.4531  0.4832  0.4354  0.5222\n",
      " 0.3949  0.4048  0.5000  0.4473  0.5498  0.2976  0.2678  0.4739  0.4678  0.4523\n",
      " 0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507\n",
      " 0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507\n",
      " 0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507\n",
      " 0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507\n",
      " 0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507\n",
      " 0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507\n",
      " 0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507\n",
      "\n",
      "Columns 50 to 59 \n",
      " 0.4771  0.5379  0.5035  0.5356  0.5178  0.4843  0.6418  0.4759  0.5476  0.4789\n",
      " 0.4825  0.5892  0.5181  0.3989  0.5864  0.4679  0.5366  0.4542  0.4507  0.4578\n",
      " 0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4578\n",
      " 0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4578\n",
      " 0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4578\n",
      " 0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4578\n",
      " 0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4578\n",
      " 0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4578\n",
      " 0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4578\n",
      " 0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4507  0.4578\n",
      "[torch.FloatTensor of size 10x60]\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "nb_out :  Variable containing:\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "[torch.FloatTensor of size 10x1]\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "162\n",
      "94\n",
      "94\n",
      "44\n",
      "31\n",
      "26\n",
      "25\n",
      "22\n",
      "17\n",
      "0\n",
      "Variable containing:\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "[torch.FloatTensor of size 10x1]\n",
      "\n",
      "rnn_out :  Variable containing:\n",
      " 0.0084  0.0810  0.1217  0.1852  0.0395 -0.0393  0.1595 -0.0432  0.1309  0.0627\n",
      " 0.0026  0.0898  0.1342  0.1766  0.0417 -0.0383  0.1438 -0.0389  0.1314  0.0655\n",
      "-0.0016  0.0970  0.1297  0.1803  0.0308 -0.0472  0.1501 -0.0392  0.1429  0.0655\n",
      " 0.0011  0.0858  0.1585  0.1845  0.0257 -0.0256  0.1636 -0.0541  0.1368  0.0549\n",
      "-0.0013  0.0787  0.1354  0.1812  0.0380 -0.0155  0.1681 -0.0451  0.1257  0.0546\n",
      "-0.0001  0.0900  0.1195  0.1756  0.0340 -0.0418  0.1454 -0.0323  0.1400  0.0749\n",
      "-0.0033  0.0879  0.1375  0.1883  0.0359 -0.0131  0.1734 -0.0607  0.1241  0.0550\n",
      " 0.0072  0.1002  0.1332  0.1783  0.0373 -0.0256  0.1636 -0.0592  0.1155  0.0518\n",
      " 0.0040  0.0776  0.1313  0.1770  0.0436 -0.0137  0.1685 -0.0546  0.1274  0.0467\n",
      " 0.0058  0.0182  0.0414  0.0731 -0.0011 -0.0188  0.0983 -0.0273  0.0692  0.0443\n",
      "[torch.FloatTensor of size 10x10]\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "cnn_out :  Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.9275  0.8191  1.0098  1.0600  0.8245  0.7433  0.9473  0.8840  0.7379  1.0416\n",
      " 0.9216  0.7955  1.0034  1.0031  0.9540  1.0234  0.9541  1.0175  1.1647  0.8272\n",
      " 1.1593  0.8837  1.0223  1.0271  0.6400  0.6521  0.8339  0.7081  0.9565  1.0894\n",
      " 0.9461  0.9075  1.0523  1.0200  1.0428  1.0112  0.9638  0.8243  0.9491  0.9236\n",
      " 0.8437  0.6973  1.1444  1.0563  0.9747  0.9079  1.0307  0.8220  1.0738  0.9454\n",
      " 1.0658  1.0439  0.7697  0.8711  1.1605  0.6990  0.9280  0.8464  0.9303  0.8448\n",
      " 0.9159  1.0086  1.1271  0.8979  1.2512  0.8174  0.5194  1.1203  0.9973  0.8400\n",
      " 0.7695  0.8282  0.8599  1.0503  1.0218  1.0142  1.0197  0.9542  1.0146  0.8473\n",
      " 0.8484  0.7240  0.8122  0.7298  0.9179  0.8936  0.8801  0.8473  0.8473  0.8473\n",
      " 0.8539  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.9750  0.8408  1.0158  0.8587  0.8930  0.7534  1.0392  0.8147  0.7919  0.8508\n",
      " 0.7009  0.9059  0.9541  0.8377  0.9864  0.8289  0.8198  1.1673  0.8414  0.7648\n",
      " 0.7708  0.7547  0.8267  0.9597  0.9256  0.7798  0.9376  0.8809  0.9812  0.5442\n",
      " 0.9828  0.8861  0.8584  0.8922  0.9673  0.6904  0.9191  0.8490  0.8473  0.8473\n",
      " 0.8092  0.9033  0.8525  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473\n",
      " 0.8288  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473\n",
      "\n",
      "Columns 20 to 29 \n",
      " 1.0776  0.9917  0.8628  0.9696  0.8665  0.9192  1.0897  1.1312  0.7856  1.1063\n",
      " 0.9551  0.6624  0.7403  1.2658  0.8022  0.6998  0.9997  0.9443  0.7053  0.7773\n",
      " 0.9236  1.1097  1.0084  0.9193  0.9792  1.0474  1.0381  0.9910  0.9638  1.0183\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473\n",
      "\n",
      "Columns 30 to 39 \n",
      " 0.9175  0.9324  1.0303  1.2577  1.0409  0.9771  1.0455  1.0030  1.0667  0.6513\n",
      " 1.0137  0.8756  0.7917  0.5151  0.8805  0.8651  0.8473  0.8473  0.8473  0.8473\n",
      " 1.0167  0.8347  1.1320  1.0054  0.9874  0.8305  0.8473  0.8473  0.8473  0.8473\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473\n",
      "\n",
      "Columns 40 to 49 \n",
      " 1.2479  0.7999  1.2697  0.8894  0.8453  0.8680  0.6935  1.1443  1.1083  1.2457\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473\n",
      "\n",
      "Columns 50 to 59 \n",
      " 0.5705  0.7500  1.1283  1.1653  0.8253  0.9967  0.8684  0.9373  1.0864  0.7991\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8526\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8526\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8526\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8526\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8526\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8526\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8526\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8526\n",
      " 0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8473  0.8526\n",
      "[torch.FloatTensor of size 10x60]\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "nb_out :  Variable containing:\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "[torch.FloatTensor of size 10x1]\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "71\n",
      "64\n",
      "46\n",
      "44\n",
      "44\n",
      "41\n",
      "31\n",
      "28\n",
      "22\n",
      "8\n",
      "Variable containing:\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "[torch.FloatTensor of size 10x1]\n",
      "\n",
      "rnn_out :  Variable containing:\n",
      " 0.0011  0.0849  0.1556  0.1892  0.0539 -0.0211  0.1607 -0.0428  0.1117  0.0387\n",
      " 0.0029  0.0850  0.1418  0.1906  0.0436 -0.0344  0.1590 -0.0350  0.1238  0.0545\n",
      " 0.0015  0.0778  0.1422  0.1854  0.0333 -0.0245  0.1583 -0.0453  0.1296  0.0599\n",
      " 0.0113  0.0981  0.1154  0.1712  0.0471 -0.0402  0.1525 -0.0391  0.1245  0.0711\n",
      "-0.0045  0.1011  0.1210  0.1781  0.0325 -0.0394  0.1695 -0.0438  0.1393  0.0651\n",
      "-0.0006  0.0827  0.1340  0.1651  0.0364 -0.0305  0.1498 -0.0501  0.1365  0.0683\n",
      " 0.0029  0.0681  0.1612  0.1798  0.0452 -0.0071  0.1642 -0.0664  0.1285  0.0384\n",
      " 0.0001  0.0808  0.1142  0.1767  0.0311 -0.0319  0.1506 -0.0452  0.1330  0.0679\n",
      " 0.0025  0.0908  0.1026  0.1657  0.0375 -0.0491  0.1355 -0.0440  0.1436  0.0768\n",
      "-0.0004  0.0911  0.1395  0.1786  0.0483 -0.0097  0.1591 -0.0525  0.1166  0.0414\n",
      "[torch.FloatTensor of size 10x10]\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "cnn_out :  Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.3600  0.3879  0.3683  0.4514  0.4192  0.4266  0.4810  0.4021  0.3841  0.3480\n",
      " 0.3877  0.3971  0.4399  0.3500  0.2886  0.2224  0.3033  0.3834  0.4887  0.4970\n",
      " 0.4767  0.4355  0.4677  0.4712  0.3943  0.4296  0.5829  0.4257  0.4100  0.5180\n",
      " 0.3833  0.3658  0.4313  0.4890  0.4609  0.4154  0.4047  0.3832  0.4879  0.5530\n",
      " 0.4217  0.4183  0.4448  0.4972  0.5087  0.2551  0.1630  0.3827  0.4368  0.4796\n",
      " 0.3872  0.3412  0.4266  0.4534  0.3736  0.5093  0.4985  0.3886  0.4855  0.4939\n",
      " 0.3964  0.4537  0.4572  0.4491  0.4439  0.5073  0.5479  0.3261  0.3065  0.3106\n",
      " 0.4969  0.4787  0.2928  0.2733  0.3346  0.4227  0.2600  0.4257  0.3825  0.3585\n",
      " 0.4556  0.4519  0.3840  0.3126  0.4884  0.4029  0.3494  0.3317  0.4156  0.4735\n",
      " 0.4566  0.3854  0.3791  0.3570  0.4111  0.4692  0.4538  0.3638  0.3594  0.3623\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.3717  0.4537  0.3271  0.4283  0.4207  0.3305  0.3970  0.4919  0.4312  0.4484\n",
      " 0.4210  0.5896  0.5153  0.2677  0.1802  0.3659  0.4044  0.2867  0.3483  0.4202\n",
      " 0.1120  0.2096  0.3104  0.3094  0.4524  0.4021  0.3899  0.5183  0.4155  0.4832\n",
      " 0.3925  0.3119  0.4346  0.4837  0.4887  0.4003  0.4293  0.4109  0.2659  0.3429\n",
      " 0.4436  0.3250  0.3726  0.4565  0.5072  0.3975  0.3766  0.3979  0.3271  0.4349\n",
      " 0.3922  0.3702  0.5144  0.4682  0.4413  0.4909  0.4280  0.3724  0.4933  0.4610\n",
      " 0.3122  0.3089  0.3576  0.3462  0.4639  0.5212  0.4344  0.4210  0.3563  0.1971\n",
      " 0.4023  0.3253  0.4636  0.4613  0.5044  0.4486  0.3662  0.4139  0.4297  0.5123\n",
      " 0.5322  0.3321  0.3330  0.4837  0.4419  0.3887  0.3253  0.3714  0.4302  0.4485\n",
      " 0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.5426  0.4788  0.3145  0.3097  0.3415  0.4412  0.4754  0.2703  0.3420  0.3969\n",
      " 0.4377  0.5350  0.4390  0.3649  0.3098  0.3978  0.4261  0.5754  0.5007  0.4474\n",
      " 0.4343  0.3605  0.4723  0.4817  0.4436  0.4871  0.3842  0.3498  0.3627  0.3033\n",
      " 0.3757  0.3353  0.3263  0.3828  0.3639  0.1634  0.2455  0.2686  0.3737  0.3071\n",
      " 0.4636  0.4672  0.4827  0.4782  0.3869  0.2814  0.3655  0.4451  0.5360  0.4690\n",
      " 0.3122  0.4916  0.5099  0.4481  0.4624  0.3300  0.4425  0.6061  0.4637  0.4680\n",
      " 0.3151  0.4371  0.4138  0.2943  0.2177  0.3123  0.4001  0.3801  0.3623  0.3623\n",
      " 0.3241  0.1812  0.3030  0.3648  0.3947  0.3405  0.3623  0.3623  0.3623  0.3623\n",
      " 0.3869  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623\n",
      " 0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623\n",
      "\n",
      "Columns 30 to 39 \n",
      " 0.4466  0.5238  0.4278  0.4604  0.4419  0.3053  0.3390  0.2715  0.3704  0.2632\n",
      " 0.4818  0.3790  0.3482  0.4921  0.3593  0.4405  0.4708  0.3978  0.1693  0.4422\n",
      " 0.2222  0.4134  0.4954  0.5235  0.4084  0.4167  0.4337  0.5494  0.3526  0.3385\n",
      " 0.3144  0.3704  0.4474  0.4407  0.4107  0.4666  0.4702  0.3211  0.3528  0.3623\n",
      " 0.4852  0.4577  0.4631  0.6080  0.3924  0.2698  0.4854  0.4422  0.3915  0.3623\n",
      " 0.4744  0.4282  0.4142  0.3301  0.3795  0.4337  0.3635  0.3623  0.3623  0.3623\n",
      " 0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623\n",
      " 0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623\n",
      " 0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623\n",
      " 0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623\n",
      "\n",
      "Columns 40 to 49 \n",
      " 0.4174  0.4102  0.4650  0.4489  0.2706  0.1679  0.3651  0.2998  0.4620  0.4014\n",
      " 0.4460  0.5152  0.4114  0.4416  0.4902  0.5107  0.4270  0.4402  0.4516  0.4096\n",
      " 0.3571  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623\n",
      " 0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623\n",
      " 0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623\n",
      " 0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623\n",
      " 0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623\n",
      " 0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623\n",
      " 0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623\n",
      " 0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623\n",
      "\n",
      "Columns 50 to 59 \n",
      " 0.3242  0.3724  0.3573  0.3866  0.5259  0.3703  0.3110  0.3703  0.3602  0.3564\n",
      " 0.3917  0.3822  0.4406  0.4461  0.4939  0.3224  0.3623  0.3623  0.3623  0.3704\n",
      " 0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3704\n",
      " 0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3704\n",
      " 0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3704\n",
      " 0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3704\n",
      " 0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3704\n",
      " 0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3704\n",
      " 0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3704\n",
      " 0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3623  0.3704\n",
      "[torch.FloatTensor of size 10x60]\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "nb_out :  Variable containing:\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "[torch.FloatTensor of size 10x1]\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "120\n",
      "89\n",
      "79\n",
      "77\n",
      "57\n",
      "51\n",
      "27\n",
      "23\n",
      "14\n",
      "0\n",
      "Variable containing:\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "[torch.FloatTensor of size 10x1]\n",
      "\n",
      "rnn_out :  Variable containing:\n",
      " 0.0054  0.0780  0.1548  0.1892  0.0425 -0.0175  0.1665 -0.0553  0.1205  0.0316\n",
      "-0.0036  0.0874  0.1397  0.1912  0.0250 -0.0339  0.1780 -0.0382  0.1394  0.0738\n",
      " 0.0036  0.1019  0.1263  0.1735  0.0439 -0.0436  0.1414 -0.0313  0.1204  0.0711\n",
      " 0.0056  0.0847  0.1470  0.1879  0.0301 -0.0294  0.1617 -0.0557  0.1323  0.0534\n",
      " 0.0087  0.0778  0.1430  0.1841  0.0402 -0.0492  0.1603 -0.0232  0.1415  0.0725\n",
      " 0.0120  0.0887  0.1225  0.1800  0.0419 -0.0430  0.1524 -0.0363  0.1247  0.0755\n",
      "-0.0045  0.0890  0.1557  0.1771  0.0290 -0.0233  0.1504 -0.0558  0.1302  0.0476\n",
      " 0.0001  0.0879  0.1482  0.1701  0.0318 -0.0214  0.1517 -0.0625  0.1231  0.0589\n",
      " 0.0056  0.0778  0.1448  0.1857  0.0232 -0.0399  0.1594 -0.0450  0.1418  0.0717\n",
      " 0.0052  0.0190  0.0436  0.0738 -0.0003 -0.0179  0.0969 -0.0256  0.0678  0.0430\n",
      "[torch.FloatTensor of size 10x10]\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "cnn_out :  Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.6089  0.6389  0.5594  0.6164  0.6824  0.8095  0.7659  0.7550  0.7085  0.7999\n",
      " 0.7345  0.9453  0.8596  0.5749  0.6456  0.9607  0.6791  0.7606  0.7317  0.7254\n",
      " 0.6612  0.5923  0.6674  0.7194  0.6693  0.7849  0.8832  0.6175  0.6586  0.5843\n",
      " 0.6646  0.6203  0.5350  0.7024  0.7813  0.8977  0.7088  0.5158  0.6591  0.6728\n",
      " 0.6539  0.5331  0.8211  0.6084  0.7112  0.5987  0.6804  0.7311  0.7542  0.7589\n",
      " 0.6164  0.6894  0.6638  0.9458  0.3277  0.8077  0.9259  0.7441  0.7393  0.5688\n",
      " 0.5873  0.7476  0.9372  0.5945  0.2721  0.8247  0.5388  0.7835  0.7157  0.4960\n",
      " 0.7381  0.6073  0.5685  0.6794  0.3999  0.6347  0.8670  0.6963  0.6949  0.5237\n",
      " 0.8157  0.6692  0.7018  0.8117  0.8408  0.5560  0.5865  0.5976  0.6184  0.6184\n",
      " 0.6266  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.6699  0.6201  0.5743  0.8681  0.5693  0.7382  0.8308  0.7753  0.6195  0.6322\n",
      " 0.9463  0.5517  1.0016  0.6671  0.7543  0.7118  0.6781  0.5659  0.6542  0.9087\n",
      " 0.7098  0.5854  0.6718  0.6403  0.9472  0.6939  0.6828  0.9498  0.7909  0.6552\n",
      " 0.6948  0.5543  0.7136  0.5142  0.9941  0.6081  0.5302  0.8824  0.5529  0.6674\n",
      " 0.6715  0.7856  0.5881  0.6813  0.7588  0.5812  0.6668  0.6450  0.7190  0.7942\n",
      " 0.6765  0.8203  0.9234  0.5352  0.5789  0.4058  0.7325  0.8318  0.5361  0.3931\n",
      " 0.7036  0.5006  0.5370  0.5530  0.6227  0.6184  0.6184  0.6184  0.6184  0.6184\n",
      " 0.6886  0.8792  0.6411  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184\n",
      " 0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184\n",
      " 0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.7123  0.6261  0.7307  0.6777  0.7279  0.7173  0.7334  0.7199  0.7793  0.7812\n",
      " 0.8651  0.7914  0.8648  0.7699  0.8619  0.7990  0.7134  0.4879  0.5398  0.7918\n",
      " 0.5685  0.6742  0.4112  0.7336  0.5235  0.7426  0.5028  0.8541  1.0040  0.6761\n",
      " 0.6918  0.7862  0.5732  0.7767  0.6316  0.7553  0.8370  0.7267  0.6955  0.3706\n",
      " 0.5630  0.5404  0.9912  0.7753  0.8091  0.7494  0.8771  0.5639  0.5826  0.7043\n",
      " 0.5145  0.5596  0.6248  0.7575  0.6501  0.6957  0.6343  0.6184  0.6184  0.6184\n",
      " 0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184\n",
      " 0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184\n",
      " 0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184\n",
      " 0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184\n",
      "\n",
      "Columns 30 to 39 \n",
      " 0.6343  0.5907  0.5177  0.7124  0.7432  0.7532  0.6560  0.6922  0.5115  0.7650\n",
      " 0.8066  0.8917  0.5797  0.5627  0.7496  0.3581  0.5724  0.5990  0.6975  0.8279\n",
      " 0.6585  0.6816  0.6018  0.5480  0.5837  0.8174  0.5372  0.9205  0.5125  0.7406\n",
      " 0.6477  0.7192  0.7212  0.6641  0.6557  0.5706  0.5881  0.5946  0.6242  0.6129\n",
      " 0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184\n",
      " 0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184\n",
      " 0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184\n",
      " 0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184\n",
      " 0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184\n",
      " 0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184\n",
      "\n",
      "Columns 40 to 49 \n",
      " 0.7129  0.9009  0.6658  0.5092  0.7477  0.6582  0.6444  0.7540  0.9798  0.6966\n",
      " 0.6631  0.7424  0.9011  0.5806  0.6589  0.6447  0.6184  0.6184  0.6184  0.6184\n",
      " 0.5880  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184\n",
      " 0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184\n",
      " 0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184\n",
      " 0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184\n",
      " 0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184\n",
      " 0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184\n",
      " 0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184\n",
      " 0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184\n",
      "\n",
      "Columns 50 to 59 \n",
      " 0.5096  0.5980  0.8391  0.5911  0.8476  0.5286  0.7776  0.6270  0.6696  0.5795\n",
      " 0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6245\n",
      " 0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6245\n",
      " 0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6245\n",
      " 0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6245\n",
      " 0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6245\n",
      " 0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6245\n",
      " 0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6245\n",
      " 0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6245\n",
      " 0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6184  0.6245\n",
      "[torch.FloatTensor of size 10x60]\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "nb_out :  Variable containing:\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "[torch.FloatTensor of size 10x1]\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "112\n",
      "86\n",
      "85\n",
      "53\n",
      "42\n",
      "35\n",
      "24\n",
      "22\n",
      "20\n",
      "14\n",
      "Variable containing:\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "[torch.FloatTensor of size 10x1]\n",
      "\n",
      "rnn_out :  Variable containing:\n",
      " 0.0051  0.0844  0.1351  0.1731  0.0411 -0.0244  0.1531 -0.0538  0.1359  0.0479\n",
      " 0.0029  0.0919  0.1391  0.1786  0.0220 -0.0369  0.1560 -0.0581  0.1421  0.0571\n",
      " 0.0108  0.0845  0.1304  0.1711  0.0483 -0.0231  0.1518 -0.0500  0.1261  0.0559\n",
      " 0.0201  0.0721  0.1072  0.1595  0.0666 -0.0250  0.1448 -0.0542  0.1256  0.0613\n",
      "-0.0004  0.1000  0.1332  0.1885  0.0361 -0.0361  0.1639 -0.0281  0.1243  0.0665\n",
      "-0.0036  0.0939  0.1381  0.1831  0.0425 -0.0283  0.1612 -0.0348  0.1136  0.0478\n",
      " 0.0055  0.1077  0.1227  0.1738  0.0558 -0.0503  0.1476 -0.0320  0.1267  0.0610\n",
      " 0.0051  0.0907  0.1569  0.1773  0.0488 -0.0095  0.1674 -0.0541  0.1086  0.0412\n",
      " 0.0086  0.0795  0.1263  0.1769  0.0335 -0.0320  0.1559 -0.0556  0.1334  0.0632\n",
      " 0.0171  0.0918  0.1160  0.1656  0.0719 -0.0377  0.1399 -0.0419  0.1208  0.0557\n",
      "[torch.FloatTensor of size 10x10]\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "cnn_out :  Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.6052  0.5414  0.6401  0.4390  0.5273  0.6360  0.8221  0.7141  0.5438  0.7421\n",
      " 0.7417  0.8197  0.6044  0.4572  0.6636  0.5185  0.4018  0.6988  0.6052  0.7317\n",
      " 0.7060  0.5545  0.6778  0.7828  0.8354  0.7643  0.5305  0.4964  0.5866  0.6145\n",
      " 0.5729  0.6802  0.5772  0.6672  0.8155  0.5177  0.5333  0.7997  0.6838  0.6761\n",
      " 0.4879  0.4898  0.6192  0.6540  0.5437  0.6624  0.8567  0.6056  0.4516  0.7017\n",
      " 0.5780  0.6751  0.8017  0.6215  0.5173  0.6005  0.6973  0.5505  0.5531  0.7041\n",
      " 0.5932  0.6900  0.6953  0.8308  0.6280  0.6209  0.6501  0.6022  0.6626  0.7200\n",
      " 0.7271  0.8139  0.6475  0.5716  0.6260  0.5304  0.7667  0.5981  0.5124  0.5876\n",
      " 0.7324  0.4981  0.6909  0.6811  0.8080  0.7280  0.7282  0.5713  0.8238  0.5711\n",
      " 0.7383  0.6300  0.5711  0.5743  0.5647  0.6786  0.6595  0.6534  0.5946  0.5791\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.5777  0.6098  0.8256  0.7211  0.6372  0.6172  0.6366  0.6029  0.6448  0.4276\n",
      " 0.4181  0.5599  0.6991  0.6908  0.5517  0.2395  0.7623  0.7929  0.7939  0.7751\n",
      " 0.6382  0.6185  0.5369  0.7231  0.7304  0.6906  0.5007  0.3961  0.7256  0.6749\n",
      " 0.5412  0.6596  0.7906  0.5835  0.6604  0.7898  0.6231  0.6233  0.5481  0.5396\n",
      " 0.7392  0.7445  0.7036  0.7923  0.8632  0.7912  0.2912  0.4457  0.6583  0.8132\n",
      " 0.4905  0.4370  0.5817  0.6657  0.7201  0.6511  0.6116  0.5583  0.5870  0.5892\n",
      " 0.5830  0.5742  0.7694  0.6146  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791\n",
      " 0.5545  0.5191  0.6000  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791\n",
      " 0.5324  0.5997  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791\n",
      " 0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.5814  0.5919  0.8403  0.5528  0.8367  0.6353  0.5604  0.5464  0.5272  0.4860\n",
      " 0.6858  0.6488  0.7130  0.7798  0.4771  0.6111  0.7710  0.7921  0.7860  0.6874\n",
      " 0.8035  0.7606  0.7316  0.7108  0.6403  0.5162  0.6371  0.6203  0.7367  0.5249\n",
      " 0.6772  0.6630  0.5116  0.5813  0.6885  0.6520  0.6597  0.4944  0.5890  0.5908\n",
      " 0.6366  0.6941  0.8498  0.5349  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791\n",
      " 0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791\n",
      " 0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791\n",
      " 0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791\n",
      " 0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791\n",
      " 0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791\n",
      "\n",
      "Columns 30 to 39 \n",
      " 0.6878  0.5467  0.5916  0.7234  0.8456  0.6167  0.5236  0.7070  0.7991  0.5895\n",
      " 0.7665  0.5809  0.7182  0.6678  0.6413  0.4886  0.5578  0.6402  0.7518  0.8635\n",
      " 0.6877  0.7038  0.6807  0.6353  0.6547  0.5365  0.6635  0.7226  0.6756  0.6405\n",
      " 0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791\n",
      " 0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791\n",
      " 0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791\n",
      " 0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791\n",
      " 0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791\n",
      " 0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791\n",
      " 0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791\n",
      "\n",
      "Columns 40 to 49 \n",
      " 0.4885  0.4715  0.7879  0.5579  0.7217  0.6303  0.6942  0.6269  0.6654  0.6668\n",
      " 0.7921  0.7814  0.7088  0.6234  0.6519  0.5156  0.6052  0.5825  0.5791  0.5791\n",
      " 0.5905  0.7233  0.6945  0.4729  0.5439  0.5085  0.5935  0.5791  0.5791  0.5791\n",
      " 0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791\n",
      " 0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791\n",
      " 0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791\n",
      " 0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791\n",
      " 0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791\n",
      " 0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791\n",
      " 0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791\n",
      "\n",
      "Columns 50 to 59 \n",
      " 0.6984  0.6558  0.6063  0.5583  0.5968  0.5780  0.6339  0.6623  0.5917  0.5840\n",
      " 0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5838\n",
      " 0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5838\n",
      " 0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5838\n",
      " 0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5838\n",
      " 0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5838\n",
      " 0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5838\n",
      " 0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5838\n",
      " 0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5838\n",
      " 0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5791  0.5838\n",
      "[torch.FloatTensor of size 10x60]\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "nb_out :  Variable containing:\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "[torch.FloatTensor of size 10x1]\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "<class 'torch.FloatTensor'> torch.Size([40, 100])\n",
      "<class 'torch.FloatTensor'> torch.Size([40, 10])\n",
      "<class 'torch.FloatTensor'> torch.Size([40])\n",
      "<class 'torch.FloatTensor'> torch.Size([40])\n",
      "<class 'torch.FloatTensor'> torch.Size([40, 10])\n",
      "<class 'torch.FloatTensor'> torch.Size([40, 10])\n",
      "<class 'torch.FloatTensor'> torch.Size([40])\n",
      "<class 'torch.FloatTensor'> torch.Size([40])\n",
      "<class 'torch.FloatTensor'> torch.Size([40, 10])\n",
      "<class 'torch.FloatTensor'> torch.Size([40, 10])\n",
      "<class 'torch.FloatTensor'> torch.Size([40])\n",
      "<class 'torch.FloatTensor'> torch.Size([40])\n",
      "<class 'torch.FloatTensor'> torch.Size([50, 100, 3])\n",
      "<class 'torch.FloatTensor'> torch.Size([50])\n",
      "<class 'torch.FloatTensor'> torch.Size([50])\n",
      "<class 'torch.FloatTensor'> torch.Size([50])\n",
      "<class 'torch.FloatTensor'> torch.Size([10, 50, 3])\n",
      "<class 'torch.FloatTensor'> torch.Size([10])\n",
      "<class 'torch.FloatTensor'> torch.Size([10])\n",
      "<class 'torch.FloatTensor'> torch.Size([10])\n",
      "<class 'torch.FloatTensor'> torch.Size([5, 10])\n",
      "<class 'torch.FloatTensor'> torch.Size([5])\n",
      "<class 'torch.FloatTensor'> torch.Size([1, 5])\n",
      "<class 'torch.FloatTensor'> torch.Size([1])\n",
      "<class 'torch.FloatTensor'> torch.Size([1, 3])\n",
      "<class 'torch.FloatTensor'> torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "formatted_list = load_object(\"../Preprocessor/save_formatted_review.pkl\")\n",
    "test_classifier = classifierModule(100, 10)\n",
    "\n",
    "test_classifier.nb_model.add_FRlist(formatted_list)\n",
    "\n",
    "batch_list = test_classifier.resize_input(formatted_list)\n",
    "for bl in batch_list:\n",
    "    test_classifier(bl)\n",
    "\n",
    "for param in test_classifier.parameters():\n",
    "     print(type(param.data), param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66590310308192335 1.0 None 0 0.6871382175959297 0.8571428571428571 0.0]\n",
      "[0.54028185322086919 1.0 None 0 0.5496398379618768 0.8571428571428571 0.0]\n",
      "[0.5797858669989947 1.0 None 0 0.8002287615745445 0.7142857142857143 0.0]\n",
      "[0.63541954286871305 0.2 None 0 0.0006383680520229973 0.5714285714285714\n",
      " 0.0]\n",
      "[0.26519822712643915 1.0 None 0 0.832603391201701 0.14285714285714285 0.0]\n",
      "[1.0000000336093371 1.0 None 0 0.8086979050931404 0.14285714285714285 0.0]\n",
      "[0.31313239764743506 0.1 None 0 0.6964236111089122 0.2857142857142857 0.0]\n",
      "[0.99999996135798463 0.2 None 0 0.007996701388037764 0.7142857142857143 0.0]\n",
      "[0.53118922964539839 0.3 None 0 0.9309861805522814 0.0 0.0]\n",
      "[0.70273453813377496 0.2 None 0 0.5445704861122067 0.8571428571428571 0.0]\n",
      "[0.99999989850489757 0.2 None 0 0.9797197453735862 0.8571428571428571 0.0]\n",
      "[0.22803039527180669 0.8 None 0 0.029895833336922806 0.14285714285714285\n",
      " 0.0]\n",
      "[1.0000001648038221 1.0 None 0 0.6441539004663355 0.8571428571428571 0.0]\n",
      "[1.0000000336093371 1.0 None 0 0.5173057291685836 0.0 0.0]\n",
      "[0.74059727450334967 0.1 None 0 0.09203576388972579 0.5714285714285714 0.0]\n",
      "[0.99999998503337451 0.2 None 0 0.7513648842577823 0.2857142857142857 0.0]\n",
      "[0.28811371257412755 1.0 None 0 0.506213634260348 0.14285714285714285 0.0]\n",
      "[0.43285142996542381 1.0 None 0 0.39938569444348104 0.5714285714285714 0.0]\n",
      "[0.99999995216771898 1.0 None 0 0.7151282754639396 0.2857142857142857 0.0]\n",
      "[0.99999997190204026 0.1 None 0 0.8531305787037127 0.42857142857142855 0.0]\n",
      "[0.44409099437478605 0.1 None 0 0.6911376851858222 0.0 0.0]\n",
      "[0.99999997724147116 0.8 None 0 0.8552350462996401 0.0 0.0]\n",
      "[1.0000001226049664 1.0 None 0 0.8655196874969988 0.14285714285714285 0.0]\n",
      "[0.99999994378383461 0.2 None 0 0.7704513657372445 0.8571428571428571 0.0]\n",
      "[0.42189092204619161 1.0 None 0 0.6823696527790162 0.7142857142857143 0.0]\n",
      "[1.0000000409556904 0.9 None 0 0.48205952546413755 0.8571428571428571 0.0]\n",
      "[0.99999998659511991 1.0 None 0 0.5026620370335877 0.7142857142857143 0.0]\n",
      "[1.0000000607414354 1.0 None 0 0.9264763541650609 0.0 0.0]\n",
      "[0.28925740656419952 1.0 None 0 0.01758122684987029 0.5714285714285714 0.0]\n",
      "[0.2433365411078966 0.9 None 0 0.7445279861130985 0.2857142857142857 0.0]\n",
      "[0.33674538492462447 1.0 None 0 0.6754798263864359 0.0 0.0]\n",
      "[0.26620488069095127 0.1 None 0 0.38570704861194827 0.7142857142857143 0.0]\n",
      "[0.42121812846305251 0.8 None 0 0.5734266435174504 0.7142857142857143 0.0]\n",
      "[0.33922169277037562 1.0 None 0 0.7994521643486223 0.7142857142857143 0.0]\n",
      "[0.35827204972919624 1.0 None 0 0.7052807754662354 0.14285714285714285 0.0]\n",
      "[1.0000000932017097 0.8 None 0 0.6815504050900927 0.14285714285714285 0.0]\n",
      "[0.42900782621013511 1.0 None 0 0.97384259258979 0.8571428571428571 0.0]\n",
      "[0.99999997354276027 1.0 None 0 0.8167023726855405 0.8571428571428571 0.0]\n",
      "[0.6376875257334399 0.4 None 0 0.6073463425927912 0.42857142857142855 0.0]\n",
      "[-1 1.0 None 0 0.8773330439798883 0.14285714285714285 0.0]\n",
      "[0.41875847217328599 1.0 None 0 0.7243039236127515 0.7142857142857143 0.0]\n",
      "[0.49503471598742688 1.0 None 0 0.5417240277747624 0.5714285714285714 0.0]\n",
      "[0.46868977205468809 0.0 None 0 0.9452546296306537 0.7142857142857143 0.0]\n",
      "[1.0000001050020531 0.9 None 0 0.6006258449051529 0.42857142857142855 0.0]\n",
      "[0.41442485776953641 0.2 None 0 0.9152537384288735 0.8571428571428571 0.0]\n",
      "[0.59811888927907508 0.2 None 0 0.7624490509260795 0.0 0.0]\n",
      "[1.0000000351782554 1.0 None 0 0.0696777662014938 0.0 0.0]\n",
      "[0.40483240316106417 0.2 None 0 0.8515371527755633 0.2857142857142857 0.0]\n",
      "[0.70790732892279051 1.0 None 0 0.9492512037031702 0.7142857142857143 0.0]\n",
      "[0.13188866793578335 1.0 None 0 0.8455845601856709 0.14285714285714285 0.0]\n",
      "[1.0000000895333667 1.0 None 0 0.8442120023173629 0.2857142857142857 0.0]\n",
      "[0.99999998778059684 1.0 None 0 0.7442592592560686 0.42857142857142855 0.0]\n",
      "[0.25250619018041331 0.2 None 0 0.9305475810178905 0.2857142857142857 0.0]\n",
      "[0.58868098632594734 1.0 None 0 0.7034685069447733 0.7142857142857143 0.0]\n",
      "[0.70280686656657032 0.2 None 0 0.8688425462969462 0.42857142857142855 0.0]\n",
      "[0.55040367593380413 1.0 None 0 0.9399523263855372 0.14285714285714285 0.0]\n",
      "[1.0000000487528609 1.0 None 0 0.32595353008946404 0.2857142857142857 0.0]\n",
      "[0.99999999852212573 1.0 None 0 0.10269675926247146 0.5714285714285714 0.0]\n",
      "[1.0000001050020531 1.0 None 0 0.6336932986087049 0.8571428571428571 0.0]\n",
      "[0.55144307089168243 0.2 None 0 0.6568632986090961 0.5714285714285714 0.0]\n",
      "[0.47594183174962157 1.0 None 0 0.8140648958360543 0.7142857142857143 0.0]\n",
      "[0.22999831848983177 1.0 None 0 0.9577662731462624 0.7142857142857143 0.0]\n",
      "[0.63638960715869475 0.2 None 0 0.6252250462930533 0.2857142857142857 0.0]\n",
      "[1.0000000336093371 1.0 None 0 0.6584103356508422 0.7142857142857143 0.0]\n",
      "[0.34999816993652733 0.2 None 0 0.7953993055562023 0.0 0.0]\n",
      "[0.3288992275387439 1.0 None 0 0.7109271527806413 0.42857142857142855 0.0]\n",
      "[1.0000000538868781 1.0 None 0 0.8888384722231422 0.14285714285714285 0.0]\n",
      "[1.0000000929814241 1.0 None 0 0.8121396180576994 0.2857142857142857 0.0]\n",
      "[-1 0.6 None 0 0.5049029629662982 0.14285714285714285 0.0]\n",
      "[0.55367979194503347 0.2 None 0 0.8613093518506503 0.7142857142857143 0.0]\n",
      "[0.4763401411810857 0.8 None 0 0.5528793981502531 0.7142857142857143 0.0]\n",
      "[0.6117514554256831 0.2 None 0 0.6341486342571443 0.2857142857142857 0.0]\n",
      "[1.0000001648038221 1.0 None 0.01593612255517406 0.8295993981446372\n",
      " 0.2857142857142857 0.0]\n",
      "[1.0000001648038221 0.8 None 0 0.6627520717593143 0.0 0.0]\n",
      "[0.60343076883224389 0.2 None 0 0.9295427314791596 0.0 0.0]\n",
      "[0.17585899005595818 1.0 None 0 0.383910740740248 0.0 0.0]\n",
      "[0.99999996500571697 0.4 None 0 0.891092673613457 0.7142857142857143 0.0]\n",
      "[0.19940346518295593 0.6 None 0 0.5041666666656965 0.42857142857142855 0.0]\n",
      "[0.99999989886421525 1.0 None 0 0.7021803935203934 0.42857142857142855 0.0]\n",
      "[1.0000000114964378 0.9 None 0 0.43330418981349794 0.5714285714285714 0.0]\n",
      "[0.23221984865942613 0.2 None 0 0.5514185995343723 0.0 0.0]\n",
      "[0.51343537638965397 1.0 None 0 0.6204170717610396 0.5714285714285714 0.0]\n",
      "[0.40007052713459601 1.0 None 0 0.04031138888967689 0.5714285714285714 0.0]\n",
      "[0.68314921536757311 0.2 None 0 0.5726627777767135 0.2857142857142857 0.0]\n",
      "[0.66282230300675793 1.0 None 0 0.8560484837944387 0.2857142857142857 0.0]\n",
      "[0.66894133417484247 0.1 None 0 0.4972326736096875 0.42857142857142855 0.0]\n",
      "[0.52031596673739322 1.0 None 0 0.3894560185217415 0.2857142857142857 0.0]\n",
      "[-1 1.0 None 0 0.6679305671277689 0.7142857142857143 0.0]\n",
      "[0.38349677593508724 1.0 None 0 0.7250572800912778 0.7142857142857143 0.0]\n",
      "[0.78959527071176749 1.0 None 0 0.436680983795668 0.7142857142857143 0.0]\n",
      "[0.99999989850489757 1.0 None 0 0.4053688078711275 0.42857142857142855 0.0]\n",
      "[0.38502519771016308 0.9 None 0 0.6327993750019232 0.8571428571428571 0.0]\n",
      "[0.62636931994013589 0.2 None 0 0.50498253472324 0.14285714285714285 0.0]\n",
      "[0.66785038214314052 1.0 None 0 0.6702063888878911 0.5714285714285714 0.0]\n",
      "[0.52443613679993561 1.0 None 0 0.8485515162028605 0.5714285714285714 0.0]\n",
      "[1.0000000713498327 1.0 None 0 0.839247662035632 0.42857142857142855 0.0]\n",
      "[1.00000006425974 1.0 None 0 0.8374496874967008 0.42857142857142855 0.0]\n",
      "[1.0000000221499641 1.0 None 0 0.9138193287071772 0.42857142857142855 0.0]\n",
      "[1.000000053148264 1.0 None 0 0.8942708333343035 0.5714285714285714 0.0]\n",
      "[0.38609248828023035 1.0 None 0 0.688505775462545 0.42857142857142855 0.0]\n"
     ]
    }
   ],
   "source": [
    "for fl in formatted_list:\n",
    "    print(fl.get_attribute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "### hyperparameters :\n",
    "\n",
    "1. learning_rate\n",
    "2. input_size\n",
    "3. rnn_output_size, cnn_output_size\n",
    "4. batch_size\n",
    "5. optimizer\n",
    "6. loss function\n",
    "7. n_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "input_size = 100  # word2vec k size\n",
    "batch_size = 100\n",
    "n_epochs = 100\n",
    "\n",
    "model = classifierModule(input_size, batch_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# target이 0일 때, p가 1-s보다 작으면 +1\n",
    "# target이 1일 때, p가 1-s보다 크면 +1\n",
    "# -> (1-s-p)*(t-1/2) <= 0 일 때 +1\n",
    "def get_accuracy(outputs, targets, sensitivity):\n",
    "    result = 0\n",
    "    t = targets.data-0.5\n",
    "    x = (1-sensitivity-outputs.data[:, 1])*t\n",
    "    for y in x:\n",
    "        if y <= 0:\n",
    "            result+=1\n",
    "    return result\n",
    "    \n",
    "def get_targets(batch):\n",
    "    targets = list()\n",
    "    for formatted in batch:\n",
    "        if formatted.label:\n",
    "            targets.append(1)\n",
    "        else:\n",
    "            targets.append(0)\n",
    "            \n",
    "    return Variable(torch.tensor(targets), requires_grad = False)\n",
    "\n",
    "def get_prediction(outputs, sensitivity):\n",
    "    return np.ceil(outputs.data[:, 1]+sensitivity-1+0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_net(self, train_list, validation_list, sensitivity = 0.5):\n",
    "    batch_list = test_classifier.resize_input(train_list)\n",
    "    \n",
    "    tacc_list = list()\n",
    "    vacc_list = list()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        tacc_list.append(0)\n",
    "        vacc_list.append(0)\n",
    "        \n",
    "        for bl in batch_list:\n",
    "            outputs = model(bl)\n",
    "            targets = get_targets(bl)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tacc_list[-1] += get_accuracy(outputs, targets, sensitivity)\n",
    "            \n",
    "        tacc_list[-1] = tacc_list[-1] / len(train_list)\n",
    "        vacc_list[-1] = get_accuracy(model(validation_list), get_targets(validation_list), sensitivity) / len(validation_list)\n",
    "    \n",
    "        print(\"epoch {}: loss.data[0] {:.3f}|  train acc {:.3f}|  validation acc {:.3f}\" \n",
    "              .format(epoch, loss.data[0], tacc_list[-1], vacc_list[-1]))\n",
    "        \n",
    "        if epoch > 5 and np.mean(np.array(tacc_list[-6:-1])) < np.mean(np.array(vacc_list[-6:-1])):\n",
    "            print(\"Seems like m1 starts to overfit, aborting training\")\n",
    "            break\n",
    "            \n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ㄹ', 'ㄷ', 'ㄴ', 'ㅁ', 'ㄱ']\n",
      "['ㄱ', 'ㅁ', 'ㄴ', 'ㄷ', 'ㄹ']\n"
     ]
    }
   ],
   "source": [
    "a = [\"ㄱ\", \"ㄴ\", \"ㄷ\", \"ㄹ\", \"ㅁ\"]\n",
    "b = [4, 3, 2, 1, 0]\n",
    "c = [4, 2, 1, 0, 3]\n",
    "\n",
    "d = [x for _,x in sorted(zip(c,a))]\n",
    "print(d)\n",
    "list.reverse(d)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [4, 5, 6], [7, 8, 9], [10]]\n"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "a_ = list()\n",
    "for i in range(0, len(a), 3):\n",
    "    a_.append(a[i:i+3])\n",
    "print(a_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
