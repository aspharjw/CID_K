{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io, os, sys, types\n",
    "from IPython import get_ipython\n",
    "from nbformat import read\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "def find_notebook(fullname, path=None):\n",
    "    \"\"\"find a notebook, given its fully qualified name and an optional path\n",
    "\n",
    "    This turns \"foo.bar\" into \"foo/bar.ipynb\"\n",
    "    and tries turning \"Foo_Bar\" into \"Foo Bar\" if Foo_Bar\n",
    "    does not exist.\n",
    "    \"\"\"\n",
    "    name = fullname.rsplit('.', 1)[-1]\n",
    "    if not path:\n",
    "        path = ['']\n",
    "    for d in path:\n",
    "        nb_path = os.path.join(d, name + \".ipynb\")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        # let import Notebook_Name find \"Notebook Name.ipynb\"\n",
    "        nb_path = nb_path.replace(\"_\", \" \")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        \n",
    "class NotebookLoader(object):\n",
    "    \"\"\"Module Loader for Jupyter Notebooks\"\"\"\n",
    "    def __init__(self, path=None):\n",
    "        self.shell = InteractiveShell.instance()\n",
    "        self.path = path\n",
    "\n",
    "    def load_module(self, fullname):\n",
    "        \"\"\"import a notebook as a module\"\"\"\n",
    "        path = find_notebook(fullname, self.path)\n",
    "\n",
    "        print (\"importing Jupyter notebook from %s\" % path)\n",
    "\n",
    "        # load the notebook object\n",
    "        with io.open(path, 'r', encoding='utf-8') as f:\n",
    "            nb = read(f, 4)\n",
    "\n",
    "\n",
    "        # create the module and add it to sys.modules\n",
    "        # if name in sys.modules:\n",
    "        #    return sys.modules[name]\n",
    "        mod = types.ModuleType(fullname)\n",
    "        mod.__file__ = path\n",
    "        mod.__loader__ = self\n",
    "        mod.__dict__['get_ipython'] = get_ipython\n",
    "        sys.modules[fullname] = mod\n",
    "\n",
    "        # extra work to ensure that magics that would affect the user_ns\n",
    "        # actually affect the notebook module's ns\n",
    "        save_user_ns = self.shell.user_ns\n",
    "        self.shell.user_ns = mod.__dict__\n",
    "\n",
    "        try:\n",
    "          for cell in nb.cells:\n",
    "            if cell.cell_type == 'code':\n",
    "                # transform the input to executable Python\n",
    "                code = self.shell.input_transformer_manager.transform_cell(cell.source)\n",
    "                # run the code in themodule\n",
    "                exec(code, mod.__dict__)\n",
    "        finally:\n",
    "            self.shell.user_ns = save_user_ns\n",
    "        return mod\n",
    "    \n",
    "class NotebookFinder(object):\n",
    "    \"\"\"Module finder that locates Jupyter Notebooks\"\"\"\n",
    "    def __init__(self):\n",
    "        self.loaders = {}\n",
    "\n",
    "    def find_module(self, fullname, path=None):\n",
    "        nb_path = find_notebook(fullname, path)\n",
    "        if not nb_path:\n",
    "            return\n",
    "\n",
    "        key = path\n",
    "        if path:\n",
    "            # lists aren't hashable\n",
    "            key = os.path.sep.join(path)\n",
    "\n",
    "        if key not in self.loaders:\n",
    "            self.loaders[key] = NotebookLoader(path)\n",
    "        return self.loaders[key]\n",
    "    \n",
    "sys.meta_path.append(NotebookFinder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from rnn.ipynb\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable   \n",
    "\n",
    "sys.path.append(\"../Preprocessor\")\n",
    "import format_module\n",
    "\n",
    "import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_object(filename):\n",
    "    with open(filename, 'rb') as input:\n",
    "        return pickle.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = 100  # word2vec k size\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class classifierModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(classifierModule, self).__init__()\n",
    "        self.rnn_model = rnn.RNN_model(input_size)\n",
    "        #TODO\n",
    "        \n",
    "    def encoder(self, formattedList):\n",
    "        length = len(formattedList)\n",
    "        contextList = [formattedList[i].context for i in range(length)]\n",
    "\n",
    "        lengths = torch.LongTensor([len(contextList[i]) for i in range(length)])\n",
    "        max_len = torch.max(lengths)\n",
    "        \n",
    "        data = np.zeros((length, max_len, input_size))\n",
    "\n",
    "        for i in range(length):\n",
    "            context = contextList[i]\n",
    "            if not (context.size == 0):\n",
    "                data[i, :context.shape[0],:] = context\n",
    "            else:\n",
    "                lengths[i] = 1\n",
    "            i+=1\n",
    "            \n",
    "        return self.sort_batch(torch.FloatTensor(data), formattedList, lengths)\n",
    "        \n",
    "    def sort_batch(self, context, formatted, seq_len):\n",
    "        batch_size = context.size(0)\n",
    "        sorted_seq_len, sorted_idx = seq_len.sort(0, descending = True)\n",
    "        \n",
    "        sorted_context = context[sorted_idx]\n",
    "        sorted_formatted = [formatted[i] for i in sorted_idx]\n",
    "\n",
    "        for f in sorted_formatted:\n",
    "            print(len(f.context))\n",
    "        \n",
    "        return Variable(sorted_context), sorted_formatted, sorted_seq_len\n",
    "    \n",
    "    def resize_input(self, input):\n",
    "        list_ = list()\n",
    "        for i in range(0, len(input), batch_size):\n",
    "            list_.append(input[i:i+batch_size])\n",
    "        return list_\n",
    "        \n",
    "    def forward(self, formatted_list, hidden=None):\n",
    "        batch_list = self.resize_input(formatted_list)\n",
    "        \n",
    "        for bl in batch_list:\n",
    "            context, formatted, lengths = self.encoder(bl)\n",
    "        \n",
    "            print(self.rnn_model(context, lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n",
      "141\n",
      "137\n",
      "124\n",
      "120\n",
      "112\n",
      "104\n",
      "96\n",
      "94\n",
      "94\n",
      "88\n",
      "86\n",
      "85\n",
      "83\n",
      "83\n",
      "73\n",
      "71\n",
      "70\n",
      "68\n",
      "63\n",
      "55\n",
      "54\n",
      "52\n",
      "49\n",
      "48\n",
      "47\n",
      "46\n",
      "45\n",
      "44\n",
      "43\n",
      "43\n",
      "42\n",
      "41\n",
      "34\n",
      "33\n",
      "33\n",
      "32\n",
      "31\n",
      "28\n",
      "28\n",
      "27\n",
      "27\n",
      "27\n",
      "27\n",
      "26\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "22\n",
      "22\n",
      "20\n",
      "20\n",
      "20\n",
      "19\n",
      "17\n",
      "16\n",
      "15\n",
      "15\n",
      "15\n",
      "13\n",
      "13\n",
      "12\n",
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "7\n",
      "6\n",
      "0\n",
      "Variable containing:\n",
      "-0.0749  0.2547 -0.0749 -0.0939  0.0044  0.2277 -0.0597  0.1489 -0.1973  0.0118\n",
      "-0.0698  0.2462 -0.0908 -0.0529  0.0186  0.2233 -0.0800  0.1419 -0.1765 -0.0335\n",
      "-0.0633  0.2417 -0.0762 -0.0731  0.0070  0.2399 -0.0730  0.1664 -0.1798 -0.0199\n",
      "-0.0787  0.2565 -0.0877 -0.0646  0.0097  0.2140 -0.0754  0.1369 -0.1853 -0.0182\n",
      "-0.0906  0.2470 -0.0854 -0.0767  0.0029  0.1876 -0.0531  0.1167 -0.1892  0.0248\n",
      "-0.0612  0.2394 -0.0835 -0.0676  0.0081  0.2258 -0.0701  0.1341 -0.1947 -0.0030\n",
      "-0.0762  0.2321 -0.0869 -0.0628  0.0140  0.2086 -0.0612  0.1201 -0.1831  0.0063\n",
      "-0.0636  0.2339 -0.0749 -0.0827  0.0086  0.2049 -0.0518  0.1242 -0.1916  0.0162\n",
      "-0.0976  0.2595 -0.0993 -0.0737 -0.0131  0.1938 -0.0641  0.1365 -0.1722 -0.0169\n",
      "-0.0865  0.2333 -0.0924 -0.0685  0.0043  0.1908 -0.0520  0.1109 -0.1878  0.0243\n",
      "-0.0698  0.2512 -0.0805 -0.0749  0.0088  0.2122 -0.0651  0.1264 -0.1991  0.0088\n",
      "-0.0713  0.2573 -0.0831 -0.0648  0.0160  0.2093 -0.0716  0.1251 -0.1968  0.0003\n",
      "-0.0698  0.2548 -0.0682 -0.0838  0.0193  0.2226 -0.0672  0.1470 -0.1958 -0.0125\n",
      "-0.0890  0.2204 -0.0825 -0.0798  0.0018  0.2105 -0.0539  0.1186 -0.1950  0.0398\n",
      "-0.0489  0.2420 -0.0608 -0.0863  0.0178  0.2375 -0.0668  0.1506 -0.1986 -0.0068\n",
      "-0.0753  0.2481 -0.0690 -0.0961  0.0016  0.2290 -0.0646  0.1551 -0.1923 -0.0097\n",
      "-0.0839  0.2592 -0.1189 -0.0477 -0.0061  0.2040 -0.0759  0.1230 -0.1802 -0.0238\n",
      "-0.0484  0.2436 -0.0719 -0.0811  0.0176  0.2380 -0.0735  0.1512 -0.2021  0.0011\n",
      "-0.0669  0.2651 -0.0716 -0.0820  0.0295  0.1983 -0.0635  0.1193 -0.2016 -0.0167\n",
      "-0.0772  0.2341 -0.0987 -0.0569  0.0084  0.2161 -0.0639  0.1322 -0.1798  0.0239\n",
      "-0.0602  0.2397 -0.0830 -0.0749  0.0054  0.2122 -0.0476  0.1330 -0.1907  0.0446\n",
      "-0.0970  0.2427 -0.1010 -0.0572  0.0002  0.2056 -0.0631  0.1235 -0.1792  0.0093\n",
      "-0.0722  0.2472 -0.0684 -0.0792  0.0158  0.2347 -0.0712  0.1525 -0.1919  0.0077\n",
      "-0.0978  0.2753 -0.0829 -0.0796  0.0082  0.2176 -0.0569  0.1488 -0.1865 -0.0056\n",
      "-0.0240  0.2363 -0.0996 -0.0554  0.0247  0.1984 -0.0663  0.1091 -0.1891  0.0102\n",
      "-0.0734  0.2348 -0.1011 -0.0469  0.0008  0.2240 -0.0770  0.1408 -0.1810 -0.0046\n",
      "-0.0728  0.2566 -0.0929 -0.0684  0.0230  0.2009 -0.0626  0.1179 -0.1992  0.0013\n",
      "-0.0662  0.2674 -0.0615 -0.0906  0.0041  0.2325 -0.0736  0.1761 -0.1897 -0.0452\n",
      "-0.0636  0.2231 -0.0986 -0.0451  0.0149  0.2247 -0.0819  0.1085 -0.1973  0.0003\n",
      "-0.0863  0.2705 -0.0805 -0.0859 -0.0004  0.1998 -0.0570  0.1329 -0.1909  0.0050\n",
      "-0.0653  0.2312 -0.0918 -0.0553  0.0308  0.2281 -0.0713  0.1282 -0.1936  0.0286\n",
      "-0.1120  0.2581 -0.0967 -0.0730 -0.0076  0.2047 -0.0672  0.1347 -0.1785 -0.0234\n",
      "-0.0724  0.2543 -0.0875 -0.0761  0.0200  0.2034 -0.0656  0.1176 -0.1998 -0.0000\n",
      "-0.0704  0.2277 -0.0750 -0.0849 -0.0013  0.2257 -0.0525  0.1417 -0.1926  0.0328\n",
      "-0.0944  0.2477 -0.0935 -0.0655  0.0100  0.2096 -0.0683  0.1243 -0.1947 -0.0056\n",
      "-0.0753  0.2447 -0.0948 -0.0658  0.0032  0.2236 -0.0622  0.1384 -0.1792  0.0161\n",
      "-0.0578  0.2467 -0.0683 -0.0799  0.0309  0.1959 -0.0641  0.1063 -0.2009 -0.0144\n",
      "-0.0573  0.2326 -0.0899 -0.0648  0.0113  0.2184 -0.0543  0.1295 -0.1909  0.0375\n",
      "-0.0643  0.2628 -0.0619 -0.0929  0.0083  0.2230 -0.0674  0.1448 -0.2075 -0.0132\n",
      "-0.0827  0.2456 -0.0885 -0.0652  0.0162  0.2108 -0.0674  0.1324 -0.1856 -0.0075\n",
      "-0.0517  0.2305 -0.1408 -0.0177  0.0178  0.1896 -0.0808  0.0948 -0.1811 -0.0100\n",
      "-0.0781  0.2581 -0.0759 -0.0805  0.0182  0.2080 -0.0620  0.1301 -0.1967  0.0097\n",
      "-0.0732  0.2575 -0.0851 -0.0663  0.0132  0.2223 -0.0696  0.1457 -0.1886 -0.0104\n",
      "-0.0693  0.2533 -0.0750 -0.0876  0.0209  0.2222 -0.0664  0.1447 -0.1925 -0.0242\n",
      "-0.0869  0.2441 -0.0883 -0.0569  0.0153  0.2372 -0.0826  0.1545 -0.1808 -0.0213\n",
      "-0.0885  0.2496 -0.1068 -0.0546 -0.0045  0.2212 -0.0712  0.1412 -0.1749 -0.0202\n",
      "-0.0299  0.2363 -0.0873 -0.0500  0.0262  0.2180 -0.0694  0.1251 -0.1907  0.0094\n",
      "-0.0828  0.2464 -0.0929 -0.0642  0.0172  0.2019 -0.0515  0.1276 -0.1844  0.0217\n",
      "-0.0665  0.2614 -0.0597 -0.1044  0.0228  0.2218 -0.0583  0.1378 -0.2083 -0.0010\n",
      "-0.0511  0.2447 -0.0642 -0.0836  0.0214  0.2489 -0.0720  0.1651 -0.1932 -0.0223\n",
      "-0.0790  0.2364 -0.1188 -0.0490  0.0026  0.1936 -0.0680  0.1100 -0.1773 -0.0064\n",
      "-0.0957  0.2642 -0.0873 -0.0693  0.0061  0.2082 -0.0770  0.1270 -0.1898 -0.0258\n",
      "-0.0455  0.2441 -0.0713 -0.0812  0.0338  0.1948 -0.0637  0.1215 -0.1993 -0.0163\n",
      "-0.0866  0.2622 -0.0714 -0.0949  0.0014  0.2248 -0.0564  0.1492 -0.1937  0.0001\n",
      "-0.0715  0.2667 -0.0899 -0.0736  0.0143  0.1900 -0.0640  0.1118 -0.1991 -0.0141\n",
      "-0.0628  0.2344 -0.1036 -0.0457  0.0167  0.2239 -0.0738  0.1258 -0.1899  0.0136\n",
      "-0.0760  0.2292 -0.0864 -0.0710  0.0151  0.2196 -0.0561  0.1216 -0.1949  0.0351\n",
      "-0.0820  0.2426 -0.0789 -0.0775 -0.0026  0.2134 -0.0585  0.1393 -0.1826  0.0055\n",
      "-0.0278  0.2352 -0.0724 -0.0843  0.0299  0.2196 -0.0571  0.1291 -0.2038  0.0163\n",
      "-0.0720  0.2519 -0.0782 -0.0850  0.0093  0.2023 -0.0507  0.1323 -0.1930  0.0123\n",
      "-0.1049  0.2456 -0.0964 -0.0633 -0.0002  0.2310 -0.0745  0.1531 -0.1676 -0.0107\n",
      "-0.0825  0.2395 -0.0823 -0.0785  0.0103  0.1854 -0.0537  0.1103 -0.1935  0.0170\n",
      "-0.0559  0.2332 -0.0612 -0.0957  0.0206  0.2118 -0.0563  0.1322 -0.2055  0.0080\n",
      "-0.0695  0.2235 -0.1167 -0.0365  0.0210  0.1975 -0.0827  0.1011 -0.1830 -0.0132\n",
      "-0.1003  0.2416 -0.0702 -0.0878 -0.0032  0.1980 -0.0519  0.1291 -0.1904  0.0173\n",
      "-0.0653  0.2347 -0.0987 -0.0448  0.0217  0.2192 -0.0750  0.1220 -0.1866  0.0017\n",
      "-0.0646  0.2540 -0.0818 -0.0677  0.0233  0.2085 -0.0686  0.1272 -0.1900 -0.0096\n",
      "-0.0893  0.2424 -0.0639 -0.0851 -0.0031  0.2039 -0.0444  0.1419 -0.1838  0.0247\n",
      "-0.1105  0.2565 -0.0524 -0.0952  0.0136  0.2067 -0.0504  0.1374 -0.1939  0.0058\n",
      "-0.0660  0.1188 -0.0334 -0.0428 -0.0225  0.0839 -0.0361  0.0480 -0.1209 -0.0014\n",
      "[torch.FloatTensor of size 70x10]\n",
      "\n",
      "120\n",
      "112\n",
      "89\n",
      "86\n",
      "85\n",
      "79\n",
      "77\n",
      "71\n",
      "64\n",
      "57\n",
      "53\n",
      "51\n",
      "46\n",
      "44\n",
      "44\n",
      "42\n",
      "41\n",
      "35\n",
      "31\n",
      "28\n",
      "27\n",
      "24\n",
      "23\n",
      "22\n",
      "22\n",
      "20\n",
      "14\n",
      "14\n",
      "8\n",
      "0\n",
      "Variable containing:\n",
      "-0.0675  0.2322 -0.0785 -0.0585  0.0167  0.2267 -0.0685  0.1386 -0.1800  0.0029\n",
      "-0.0489  0.2367 -0.0880 -0.0622  0.0131  0.2375 -0.0823  0.1467 -0.1976 -0.0092\n",
      "-0.1023  0.2544 -0.0907 -0.0764 -0.0078  0.2182 -0.0711  0.1447 -0.1813 -0.0174\n",
      "-0.0801  0.2586 -0.0722 -0.0956  0.0174  0.2275 -0.0578  0.1624 -0.1944 -0.0028\n",
      "-0.1040  0.2508 -0.0937 -0.0687 -0.0052  0.2122 -0.0648  0.1418 -0.1671 -0.0236\n",
      "-0.0748  0.2511 -0.0537 -0.0956  0.0147  0.2242 -0.0526  0.1535 -0.1936  0.0075\n",
      "-0.0713  0.2493 -0.0791 -0.0773  0.0144  0.2287 -0.0691  0.1421 -0.1949  0.0011\n",
      "-0.1177  0.2599 -0.0997 -0.0829 -0.0253  0.1939 -0.0526  0.1419 -0.1696 -0.0016\n",
      "-0.1135  0.2512 -0.1044 -0.0679 -0.0246  0.2061 -0.0629  0.1386 -0.1757  0.0002\n",
      "-0.0629  0.2574 -0.0941 -0.0627  0.0146  0.2105 -0.0580  0.1402 -0.1838  0.0152\n",
      "-0.0699  0.2319 -0.1172 -0.0409  0.0095  0.2037 -0.0744  0.1157 -0.1794  0.0004\n",
      "-0.0661  0.2494 -0.0553 -0.1032  0.0219  0.2231 -0.0580  0.1364 -0.2101  0.0090\n",
      "-0.0629  0.2438 -0.0783 -0.0833  0.0235  0.2082 -0.0530  0.1319 -0.1951  0.0229\n",
      "-0.0639  0.2164 -0.1091 -0.0332  0.0400  0.2180 -0.0836  0.0983 -0.1856 -0.0111\n",
      "-0.1130  0.2717 -0.0999 -0.0634 -0.0058  0.1972 -0.0696  0.1288 -0.1814 -0.0317\n",
      "-0.0469  0.2301 -0.0623 -0.0885  0.0326  0.2443 -0.0635  0.1569 -0.1918  0.0151\n",
      "-0.0893  0.2480 -0.0663 -0.0770  0.0074  0.2334 -0.0632  0.1544 -0.1858  0.0041\n",
      "-0.0724  0.2560 -0.0901 -0.0583  0.0130  0.2144 -0.0690  0.1412 -0.1782 -0.0052\n",
      "-0.0901  0.2587 -0.0654 -0.1051 -0.0040  0.2087 -0.0561  0.1409 -0.1988 -0.0049\n",
      "-0.0961  0.2603 -0.0820 -0.0812  0.0003  0.2045 -0.0585  0.1369 -0.1919  0.0057\n",
      "-0.0844  0.2596 -0.0516 -0.1008  0.0038  0.2484 -0.0663  0.1716 -0.2015 -0.0089\n",
      "-0.1219  0.2701 -0.0725 -0.0827 -0.0176  0.2174 -0.0701  0.1509 -0.1861 -0.0383\n",
      "-0.0450  0.2347 -0.0733 -0.0671  0.0320  0.2330 -0.0746  0.1326 -0.1964 -0.0134\n",
      "-0.0753  0.2307 -0.0867 -0.0558  0.0360  0.2185 -0.0712  0.1237 -0.1859  0.0019\n",
      "-0.0672  0.2590 -0.0737 -0.0761  0.0198  0.2180 -0.0645  0.1418 -0.1945  0.0003\n",
      "-0.1171  0.2397 -0.1118 -0.0526 -0.0130  0.2152 -0.0707  0.1364 -0.1697 -0.0011\n",
      "-0.0966  0.2468 -0.1050 -0.0661 -0.0097  0.1877 -0.0566  0.1259 -0.1786  0.0089\n",
      "-0.0904  0.2666 -0.0954 -0.0710 -0.0068  0.1965 -0.0665  0.1333 -0.1814 -0.0327\n",
      "-0.0798  0.2419 -0.0959 -0.0484  0.0135  0.2234 -0.0784  0.1319 -0.1869 -0.0035\n",
      "-0.0709  0.1187 -0.0331 -0.0443 -0.0221  0.0809 -0.0373  0.0452 -0.1239 -0.0030\n",
      "[torch.FloatTensor of size 30x10]\n",
      "\n",
      "<class 'torch.FloatTensor'> torch.Size([40, 100])\n",
      "<class 'torch.FloatTensor'> torch.Size([40, 10])\n",
      "<class 'torch.FloatTensor'> torch.Size([40])\n",
      "<class 'torch.FloatTensor'> torch.Size([40])\n",
      "<class 'torch.FloatTensor'> torch.Size([40, 10])\n",
      "<class 'torch.FloatTensor'> torch.Size([40, 10])\n",
      "<class 'torch.FloatTensor'> torch.Size([40])\n",
      "<class 'torch.FloatTensor'> torch.Size([40])\n",
      "<class 'torch.FloatTensor'> torch.Size([40, 10])\n",
      "<class 'torch.FloatTensor'> torch.Size([40, 10])\n",
      "<class 'torch.FloatTensor'> torch.Size([40])\n",
      "<class 'torch.FloatTensor'> torch.Size([40])\n"
     ]
    }
   ],
   "source": [
    "formatted_list = load_object(\"../Preprocessor/save_formatted_review.pkl\")\n",
    "test_classifier = classifierModule()\n",
    "\n",
    "#print(formatted_list[0].context.shape)\n",
    "test_classifier(formatted_list)\n",
    "\n",
    "for param in test_classifier.parameters():\n",
    "     print(type(param.data), param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ㄹ', 'ㄷ', 'ㄴ', 'ㅁ', 'ㄱ']\n",
      "['ㄱ', 'ㅁ', 'ㄴ', 'ㄷ', 'ㄹ']\n"
     ]
    }
   ],
   "source": [
    "a = [\"ㄱ\", \"ㄴ\", \"ㄷ\", \"ㄹ\", \"ㅁ\"]\n",
    "b = [4, 3, 2, 1, 0]\n",
    "c = [4, 2, 1, 0, 3]\n",
    "\n",
    "d = [x for _,x in sorted(zip(c,a))]\n",
    "print(d)\n",
    "list.reverse(d)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [4, 5, 6], [7, 8, 9], [10]]\n"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "a_ = list()\n",
    "for i in range(0, len(a), 3):\n",
    "    a_.append(a[i:i+3])\n",
    "print(a_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
