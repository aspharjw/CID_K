{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io, os, sys, types\n",
    "from IPython import get_ipython\n",
    "from nbformat import read\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "def find_notebook(fullname, path=None):\n",
    "    \"\"\"find a notebook, given its fully qualified name and an optional path\n",
    "\n",
    "    This turns \"foo.bar\" into \"foo/bar.ipynb\"\n",
    "    and tries turning \"Foo_Bar\" into \"Foo Bar\" if Foo_Bar\n",
    "    does not exist.\n",
    "    \"\"\"\n",
    "    name = fullname.rsplit('.', 1)[-1]\n",
    "    if not path:\n",
    "        path = ['']\n",
    "    for d in path:\n",
    "        nb_path = os.path.join(d, name + \".ipynb\")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        # let import Notebook_Name find \"Notebook Name.ipynb\"\n",
    "        nb_path = nb_path.replace(\"_\", \" \")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        \n",
    "class NotebookLoader(object):\n",
    "    \"\"\"Module Loader for Jupyter Notebooks\"\"\"\n",
    "    def __init__(self, path=None):\n",
    "        self.shell = InteractiveShell.instance()\n",
    "        self.path = path\n",
    "\n",
    "    def load_module(self, fullname):\n",
    "        \"\"\"import a notebook as a module\"\"\"\n",
    "        path = find_notebook(fullname, self.path)\n",
    "\n",
    "        print (\"importing Jupyter notebook from %s\" % path)\n",
    "\n",
    "        # load the notebook object\n",
    "        with io.open(path, 'r', encoding='utf-8') as f:\n",
    "            nb = read(f, 4)\n",
    "\n",
    "\n",
    "        # create the module and add it to sys.modules\n",
    "        # if name in sys.modules:\n",
    "        #    return sys.modules[name]\n",
    "        mod = types.ModuleType(fullname)\n",
    "        mod.__file__ = path\n",
    "        mod.__loader__ = self\n",
    "        mod.__dict__['get_ipython'] = get_ipython\n",
    "        sys.modules[fullname] = mod\n",
    "\n",
    "        # extra work to ensure that magics that would affect the user_ns\n",
    "        # actually affect the notebook module's ns\n",
    "        save_user_ns = self.shell.user_ns\n",
    "        self.shell.user_ns = mod.__dict__\n",
    "\n",
    "        try:\n",
    "          for cell in nb.cells:\n",
    "            if cell.cell_type == 'code':\n",
    "                # transform the input to executable Python\n",
    "                code = self.shell.input_transformer_manager.transform_cell(cell.source)\n",
    "                # run the code in themodule\n",
    "                exec(code, mod.__dict__)\n",
    "        finally:\n",
    "            self.shell.user_ns = save_user_ns\n",
    "        return mod\n",
    "    \n",
    "class NotebookFinder(object):\n",
    "    \"\"\"Module finder that locates Jupyter Notebooks\"\"\"\n",
    "    def __init__(self):\n",
    "        self.loaders = {}\n",
    "\n",
    "    def find_module(self, fullname, path=None):\n",
    "        nb_path = find_notebook(fullname, path)\n",
    "        if not nb_path:\n",
    "            return\n",
    "\n",
    "        key = path\n",
    "        if path:\n",
    "            # lists aren't hashable\n",
    "            key = os.path.sep.join(path)\n",
    "\n",
    "        if key not in self.loaders:\n",
    "            self.loaders[key] = NotebookLoader(path)\n",
    "        return self.loaders[key]\n",
    "    \n",
    "sys.meta_path.append(NotebookFinder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydblite in c:\\users\\shako\\anaconda3\\lib\\site-packages\n"
     ]
    }
   ],
   "source": [
    "!pip install pydblite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable   \n",
    "from torch import optim\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../Preprocessor\")\n",
    "import format_module\n",
    "reviewDB = format_module.FormattedReview.reviewDB\n",
    "\n",
    "import rnn\n",
    "import naivebayesian\n",
    "import cnn\n",
    "import conclude\n",
    "import mlp\n",
    "\n",
    "reviewDB = format_module.ReviewDB(\"../Preprocessor/pkl/train\")\n",
    "format_module.FormattedReview.setDB(reviewDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_object(filename):\n",
    "    with open(filename, 'rb') as input:\n",
    "        return pickle.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class classifierModule(nn.Module):\n",
    "    def __init__(self, input_size, batch_size, FRlist, path, refresh = False):\n",
    "        super(classifierModule, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.rnn_model = rnn.RNN_model(input_size)\n",
    "        self.rnn_out_size = rnn.RNN_model.hidden_size\n",
    "        self.rnn_mlp = mlp.mlp(format_module.FormattedReview.attribute_num, self.rnn_out_size)\n",
    "        \n",
    "        self.nb_model = naivebayesian.NaiveBayesianDB()\n",
    "        self.nb_out_size = 1\n",
    "        self.nb_mlp = mlp.mlp(format_module.FormattedReview.attribute_num, self.nb_out_size)\n",
    "        \n",
    "        self.cnn_model = cnn.ConvNet(input_size)\n",
    "        self.cnn_out_size = cnn.ConvNet.output_vector_size\n",
    "        self.cnn_mlp = mlp.mlp(format_module.FormattedReview.attribute_num, self.cnn_out_size)\n",
    "        \n",
    "        self.conclude = conclude.conclude()\n",
    "        \n",
    "        self.nb_model.add_FRlist(FRlist) #initialize nb database\n",
    "        \n",
    "        self.path = path\n",
    "        \n",
    "        if os.path.exists(path) and not refresh:\n",
    "            self.load_state_dict(torch.load(path))\n",
    "            \n",
    "        else:\n",
    "            self.save_state_dict()\n",
    "        \n",
    "    def save_state_dict(self):\n",
    "        torch.save(self.state_dict(), self.path)\n",
    "        \n",
    "    def encoder(self, formattedList):\n",
    "        length = len(formattedList)\n",
    "        contextList = [formattedList[i].context for i in range(length)]\n",
    "\n",
    "        lengths = torch.LongTensor([len(contextList[i]) for i in range(length)])\n",
    "        max_len = torch.max(lengths)\n",
    "        \n",
    "        data = np.zeros((length, max_len, self.input_size))\n",
    "\n",
    "        for i in range(length):\n",
    "            context = contextList[i]\n",
    "            if not (context.size == 0):\n",
    "                data[i, :context.shape[0],:] = context\n",
    "            else:\n",
    "                lengths[i] = 1\n",
    "            \n",
    "        return self.sort_batch(torch.FloatTensor(data), formattedList, lengths)\n",
    "        \n",
    "    def sort_batch(self, context, formatted, seq_len):\n",
    "        batch_size = context.size(0)\n",
    "        sorted_seq_len, sorted_idx = seq_len.sort(0, descending = True)\n",
    "        \n",
    "        sorted_context = context[sorted_idx]\n",
    "        sorted_formatted = [formatted[i] for i in sorted_idx]\n",
    "        \n",
    "        return Variable(sorted_context), sorted_formatted, sorted_seq_len\n",
    "    \n",
    "    def resize_input(self, input):\n",
    "        list_ = list()\n",
    "        for i in range(0, len(input), self.batch_size):\n",
    "            list_.append(input[i:i+self.batch_size])\n",
    "        return list_\n",
    "        \n",
    "    def forward(self, formatted_list, hidden=None, mode = \"Default\"):\n",
    "        context, formatted, lengths = self.encoder(formatted_list)\n",
    "        \n",
    "        if mode == \"rnn\":\n",
    "            rnn_out = self.rnn_model(context, lengths)\n",
    "            rnn_mlp_out = self.rnn_mlp(self.rnn_mlp.getdata(formatted, rnn_out))\n",
    "            output_0to1 = torch.nn.functional.sigmoid(rnn_mlp_out)\n",
    "            return torch.cat([1- output_0to1, output_0to1],1)\n",
    "            \n",
    "        elif mode == \"cnn\":\n",
    "            cnn_out = self.cnn_model(context)\n",
    "            cnn_mlp_out = self.cnn_mlp(self.cnn_mlp.getdata(formatted, cnn_out))\n",
    "            output_0to1 = torch.nn.functional.sigmoid(cnn_mlp_out)\n",
    "            return torch.cat([1- output_0to1, output_0to1],1)\n",
    "            \n",
    "        elif mode == \"nb\":\n",
    "            nb_out = self.nb_model.naive_bayes_FRlist(formatted)\n",
    "            nb_mlp_out = self.nb_mlp(self.nb_mlp.getdata(formatted, nb_out))\n",
    "            output_0to1 = torch.nn.functional.sigmoid(nb_mlp_out)\n",
    "            return torch.cat([1- output_0to1, output_0to1],1)\n",
    "            \n",
    "        else:\n",
    "            rnn_out = self.rnn_model(context, lengths)\n",
    "            cnn_out = self.cnn_model(context)\n",
    "            nb_out = self.nb_model.naive_bayes_FRlist(formatted)\n",
    "            \n",
    "            rnn_mlp_out = self.rnn_mlp(self.rnn_mlp.getdata(formatted, rnn_out))\n",
    "            cnn_mlp_out = self.cnn_mlp(self.cnn_mlp.getdata(formatted, cnn_out))\n",
    "            nb_mlp_out = self.nb_mlp(self.nb_mlp.getdata(formatted, nb_out))\n",
    "            \n",
    "            return self.conclude(self.conclude.bind(rnn_mlp_out, cnn_mlp_out, nb_mlp_out))\n",
    "        \n",
    "        '''\n",
    "        print(\"rnn_out : \", rnn_out.size())\n",
    "        print(\"cnn_out : \", cnn_out.size())\n",
    "        print(\"nb_out : \", nb_out.size(), \"\\n\")\n",
    "        '''\n",
    "        \n",
    "    def print_contribution(self):\n",
    "        (weight, bias) = self.conclude.get_contribution()\n",
    "        print(\"----------------- Current model contribution ----------------\")\n",
    "        print(\"-- rnn : \", weight.data[0][0])\n",
    "        print(\"-- cnn : \", weight.data[0][1])\n",
    "        print(\"-- nb : \", weight.data[0][2])\n",
    "        print(\"-- bias : \", bias.data[0])\n",
    "        print(\"-------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "formatted_list = load_object(\"../Preprocessor/pkl/save_formatted_review_validation.pkl\")\n",
    "test_classifier = classifierModule(100, 100, formatted_list, \"./models/asdf.mdl\",True)\n",
    "\n",
    "batch_list = test_classifier.resize_input(formatted_list)\n",
    "#for bl in batch_list:\n",
    "#    print(test_classifier(bl, 'rnn'))\n",
    "\n",
    "test_classifier.print_contribution()\n",
    "'''\n",
    "for param in test_classifier.parameters():\n",
    "     print(type(param.data), param.size())\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for fl in formatted_lisrt:\n",
    "    print(fl.get_attribute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "### hyperparameters :\n",
    "\n",
    "1. learning_rate\n",
    "2. input_size\n",
    "3. rnn_output_size, cnn_output_size\n",
    "4. batch_size\n",
    "5. optimizer\n",
    "6. loss function\n",
    "7. n_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "input_size = 100  # word2vec k size\n",
    "batch_size = 100\n",
    "n_epochs = 40"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "reviewDB = format_module.ReviewDB(\"../Preprocessor/pkl/train\")\n",
    "format_module.FormattedReview.setDB(reviewDB)\n",
    "FRlist = load_object(\"../Preprocessor/pkl/save_formatted_review_train.pkl\")[:100]\n",
    "model = classifierModule(input_size, batch_size, FRlist, \"./models/test_model.mdl\", True)\n",
    "criterion = nn.CrossEntropyLoss(torch.FloatTensor([1,6]))\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# target이 0일 때, p가 1-s보다 작으면 +1\n",
    "# target이 1일 때, p가 1-s보다 크면 +1\n",
    "# -> (1-s-p)*(t-1/2) <= 0 일 때 +1\n",
    "def get_accuracy(outputs, targets, sensitivity):\n",
    "    result = 0\n",
    "    t = targets.data.type(torch.FloatTensor)-0.5\n",
    "    x = (1-sensitivity-outputs.data[:, 1])*t\n",
    "    for y in x:\n",
    "        if y < 0:\n",
    "            result+=1\n",
    "    return result\n",
    "    \n",
    "def get_targets(input, model):\n",
    "    _, batch, _ = model.encoder(input)\n",
    "    targets = list()\n",
    "    for formatted in batch:\n",
    "        if formatted.label:\n",
    "            targets.append(1)\n",
    "        else:\n",
    "            targets.append(0)\n",
    "            \n",
    "    return Variable(torch.LongTensor(targets), requires_grad = False)\n",
    "\n",
    "def get_prediction(outputs, sensitivity):\n",
    "    return np.ceil(outputs.data[:, 1]+sensitivity-1+0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_net(train_list, validation_list, sensitivity = 0.5, run_mode = \"default\"):\n",
    "    batch_list = model.resize_input(train_list)\n",
    "    \n",
    "    tacc_list = list()\n",
    "    vacc_list = list()\n",
    "    \n",
    "    for input in train_list:\n",
    "        reviewDB.add_spam_result(input.bookingReview.id, input.label)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        tacc_list.append(0)\n",
    "        vacc_list.append(0)\n",
    "        \n",
    "        for bl in batch_list:\n",
    "            outputs = model(bl, mode = run_mode)\n",
    "            targets = get_targets(bl, model)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tacc_list[-1] += get_accuracy(outputs, targets, sensitivity)\n",
    "            \n",
    "        tacc_list[-1] = tacc_list[-1] / len(train_list)\n",
    "        \n",
    "        v_outputs = model(validation_list, mode = run_mode)\n",
    "        v_targets = get_targets(validation_list, model)\n",
    "        vacc_list[-1] = get_accuracy(v_outputs, v_targets, sensitivity) / len(validation_list)\n",
    "        v_loss = criterion(v_outputs, v_targets)\n",
    "    \n",
    "        print(\"epoch {}: train acc {:.6f} | validation acc {:.6f}\" \n",
    "              .format(epoch, tacc_list[-1], vacc_list[-1]))\n",
    "        print(\"------- loss.data   {:.6f} | v_loss.data    {:.6f}\"\n",
    "              .format(loss.data[0], v_loss.data[0]))\n",
    "        \n",
    "        #if epoch > 5 and np.mean(np.array(tacc_list[-6:-1])) < np.mean(np.array(vacc_list[-6:-1])):\n",
    "        #    print(\"Seems like m1 starts to overfit, aborting training\")\n",
    "        #    break\n",
    "            \n",
    "        model.save_state_dict()\n",
    "            \n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "train_list = load_object(\"../Preprocessor/pkl/save_formatted_review_train.pkl\")\n",
    "validation_list = load_object(\"../Preprocessor/pkl/save_formatted_review_validation.pkl\")\n",
    "train_net(train_list, validation_list)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "test_list = load_object(\"../Preprocessor/pkl/save_formatted_review_test.pkl\")\n",
    "out = model(test_list, mode = 'default')\n",
    "tg = get_targets(test_list, model)\n",
    "_, formatted, _ = model.encoder(test_list)\n",
    "\n",
    "print(get_accuracy(out, tg, 0.5)/len(test_list))\n",
    "for i in range(1000):\n",
    "    acc = get_accuracy(out[i:i+1], tg[i:i+1], 0.5)\n",
    "    print(\"\\n\", formatted[i].review_id)\n",
    "    if tg.data[i]:\n",
    "        print(\"            \",out.data[i, 1], tg.data[i], acc)\n",
    "    else:\n",
    "        print(out.data[i, 1], tg.data[i], acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spamFilterModule, which incorporates both preprocessing and classifying.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import preprocessorModule\n",
    "\n",
    "\n",
    "def spamFilterModule(train_or_not, input_excel_file_path,input_excel_file_for_validation_path = None,\n",
    "                     sensitivity = 0.5, run_mode = 'default'):\n",
    "    if(train_or_not):\n",
    "        if(input_excel_file_for_validation_path == None): \n",
    "            print(\"no excel for validation.. abort training\")\n",
    "            return\n",
    "        else:\n",
    "            formatted_review_list_for_training = preprocessorModule.preprocessModule(input_excel_file_path, reviewDB, \"train\")\n",
    "            formatted_review_list_for_validating = preprocessorModule.preprocessModule(input_excel_file_for_validation_path, reviewDB, \"validation\")\n",
    "            train_net(formatted_review_list_for_training,formatted_review_list_for_validating,sensitivity,run_mode)\n",
    "    else:\n",
    "        formatted_review_list = preprocessorModule.preprocessModule(input_excel_file_path, reviewDB, \"test\")\n",
    "        out = model(formatted_review_list, None,run_mode)        \n",
    "        tg = get_targets(formatted_review_list, model)\n",
    "        _, formatted, _ = model.encoder(formatted_review_list)\n",
    "        print()\n",
    "        print(\"############# print test accuracy and infer results for each ##############\")\n",
    "        print(\"%5.3f\" % (100 * get_accuracy(out, tg, 0.5)/len(formatted_review_list)),end = \"\")\n",
    "        print(\"%\")\n",
    "        for i in range(500):\n",
    "            acc = get_accuracy(out[i:i+1], tg[i:i+1], 0.5)\n",
    "            if tg.data[i]:\n",
    "                print(\"            \",out.data[i, 1], tg.data[i], acc)\n",
    "            else:\n",
    "                print(out.data[i, 1], tg.data[i], acc)\n",
    "            "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "spamFilterModule(False,\"../Preprocessor/Commonreviews_snuproject _test.xlsx\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "spamFilterModule(True,\"../Preprocessor/Commonreviews_snuproject_train.xlsx\",\n",
    "                 \"../Preprocessor/Commonreviews_snuproject_validation.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code from\n",
    "https://github.com/szagoruyko/functional-zoo/blob/master/visualize.py"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from graphviz import Digraph\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "def make_dot(var, params=None):\n",
    "    \"\"\" Produces Graphviz representation of PyTorch autograd graph\n",
    "    Blue nodes are the Variables that require grad, orange are Tensors\n",
    "    saved for backward in torch.autograd.Function\n",
    "    Args:\n",
    "        var: output Variable\n",
    "        params: dict of (name, Variable) to add names to node that\n",
    "            require grad (TODO: make optional)\n",
    "    \"\"\"\n",
    "    if params is not None:\n",
    "        assert isinstance(params.values()[0], Variable)\n",
    "        param_map = {id(v): k for k, v in params.items()}\n",
    "\n",
    "    node_attr = dict(style='filled',\n",
    "                     shape='box',\n",
    "                     align='left',\n",
    "                     fontsize='12',\n",
    "                     ranksep='0.1',\n",
    "                     height='0.2')\n",
    "    dot = Digraph(node_attr=node_attr, graph_attr=dict(size=\"12,12\"))\n",
    "    seen = set()\n",
    "\n",
    "    def size_to_str(size):\n",
    "        return '('+(', ').join(['%d' % v for v in size])+')'\n",
    "\n",
    "    def add_nodes(var):\n",
    "        if var not in seen:\n",
    "            if torch.is_tensor(var):\n",
    "                dot.node(str(id(var)), size_to_str(var.size()), fillcolor='orange')\n",
    "            elif hasattr(var, 'variable'):\n",
    "                u = var.variable\n",
    "                name = param_map[id(u)] if params is not None else ''\n",
    "                #node_name = '%s\\n %s' % (name, size_to_str(u.size()))\n",
    "                node_name = '%s' % (name)\n",
    "                dot.node(str(id(var)), node_name, fillcolor='lightblue')\n",
    "            else:\n",
    "                dot.node(str(id(var)), str(type(var).__name__))\n",
    "            seen.add(var)\n",
    "            if hasattr(var, 'next_functions'):\n",
    "                for u in var.next_functions:\n",
    "                    if u[0] is not None:\n",
    "                        dot.edge(str(id(u[0])), str(id(var)))\n",
    "                        add_nodes(u[0])\n",
    "            if hasattr(var, 'saved_tensors'):\n",
    "                for t in var.saved_tensors:\n",
    "                    dot.edge(str(id(t)), str(id(var)))\n",
    "                    add_nodes(t)\n",
    "    add_nodes(var.grad_fn)\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "out2 = test_classifier(load_object(\"../Preprocessor/pkl/save_formatted_review_validation.pkl\")[0:2])\n",
    "print(out2.size())\n",
    "\n",
    "print(1)\n",
    "a = make_dot(out2)\n",
    "print(2)\n",
    "#a.render('graph.pdf', view=True)\n",
    "print(3)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "a = [\"ㄱ\", \"ㄴ\", \"ㄷ\", \"ㄹ\", \"ㅁ\"]\n",
    "b = [4, 3, 2, 1, 0]\n",
    "c = [4, 2, 1, 0, 3]\n",
    "\n",
    "d = [x for _,x in sorted(zip(c,a))]\n",
    "print(d)\n",
    "list.reverse(d)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "a_ = list()\n",
    "for i in range(0, len(a), 3):\n",
    "    a_.append(a[i:i+3])\n",
    "print(a_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
